{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F80EwwEaEIm4",
        "outputId": "7a3c553d-8b1a-4ce5-b088-af97c49e0e5b"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIR-5l3qD46l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94yp7HDkEQOx",
        "outputId": "843e8d09-8eb7-490b-ea89-0f4b85daf300"
      },
      "outputs": [],
      "source": [
        "rn_torch = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ-jeGGoWsPj",
        "outputId": "fc723e06-a1b3-4938-a708-1ce6ecda2829"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UNNYAUbAeY"
      },
      "source": [
        "# 2.2.1\n",
        "\n",
        "ImageNet uses the WordNet hierarchy for organizing its 1000 object categories. WordNet is a lexical database that groups words into synonym sets (synsets) and connects them through various relationships (is-a, part-of, etc.). In ImageNet, each synset corresponds to a specific object category, forming a many-to-one mapping (multiple words in a synset can describe the same object). The hierarchy is a directed acyclic graph (DAG) where each node represents a synset and each edge represents a relationship between synsets. The root node is the synset \"entity\" and the leaves are the object categories. The hierarchy is used to define the label space for the ImageNet dataset, where each image is labeled with the synset of the object it contains.\n",
        "\n",
        "# 2.2.2\n",
        "\n",
        "A synset in WordNet is a set of synonyms that represent the same underlying concept or idea. In the context of ImageNet, a synset refers to a specific object category. For example, the synset \"n01531178: terrier\" represents the category of terrier dogs.\n",
        "\n",
        "# 2.2.3\n",
        "\n",
        "Yes, grouping objects based solely on synsets can lead to challenges in visual recognition for a few reasons:\n",
        "*  A single word can have multiple meanings (synsets) depending on context. For example, \"bat\" can refer to the flying mammal (synset for animals) or a baseball bat (synset for sports equipment). ImageNet relies on the surrounding words or image content for disambiguation, which might not always be available.\n",
        "* Synsets can be broad or narrow, leading to variations in object categories. For example, the synset \"n01531178: terrier\" is more specific than the synset \"n02085620: Chihuahua,\" which is a type of terrier. This can affect the granularity of object recognition.\n",
        "* Synsets might group visually distinct objects with similar overall concepts. For instance, the synset \"n02121808: golden retriever\" might encompass images of golden retrievers in various poses, fur colors, or even with different objects like frisbees.\n",
        "* Synsets primarily focus on the object category itself and might not capture specific attributes like size, color, or material. These attributes can be crucial for recognizing specific objects within a synset.\n",
        "\n",
        "# 2.2.4\n",
        "\n",
        "1. Pose and Viewpoint: Objects within a category can appear in different poses (standing, sitting, lying down) or from various viewpoints (side view, front view, etc.).\n",
        "2. Lighting and Background: Lighting conditions and background clutter can significantly alter the appearance of objects within a category.\n",
        "3. Object Attributes: Objects belonging to the same category might exhibit variations in attributes like size, color, or material (e.g., different colored sneakers within the \"sneaker\" synset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w1Kb-Hya_nz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "url = 'https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt'\n",
        "class_names = requests.get(url).text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6HJw3Gp3raj"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvotiW102x9R"
      },
      "outputs": [],
      "source": [
        "img_path= '/content/ILSVRC2012_val_00042079.JPEG'\n",
        "img = Image.open(img_path)\n",
        "image = preprocess(img).unsqueeze(0).to(device)\n",
        "text_t = torch.cat([clip.tokenize(f\"a {c}\") for c in class_names]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGstHwAU3wa6",
        "outputId": "57b17c4d-090d-4c95-917b-5637254224b4"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_inputs = [f\"a {class_name}\" for class_name in class_names]\n",
        "    text_input = clip.tokenize(text_inputs).to(device)\n",
        "    logits_per_image, logits_per_text = model(image, text_input)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "top5_probs, top5_classes = torch.topk(logits_per_image, 5)\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Class: {class_names[top5_classes[0][i]]}, Probability: {top5_probs[0][i].item()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_uLjQ4H4Zv3"
      },
      "source": [
        "Working as expected and is able to predict the correct object category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "GGJZmgGH4hEy",
        "outputId": "8845e590-994a-4c2d-a983-e5ee8e311962"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "def display_image_and_categories(img_path,model = model):\n",
        "    # Load and preprocess the image\n",
        "    img = Image.open(img_path)\n",
        "    image = preprocess(img).unsqueeze(0).to(device)\n",
        "    img_t = transform(img).unsqueeze(0)\n",
        "\n",
        "\n",
        "    # Perform the inference\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(\"cuda\"):\n",
        "          with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof:\n",
        "            # with record_function(\"model_inference\"):\n",
        "                  image_features = model.encode_image(image)\n",
        "                  text_inputs = [f\"a {class_name}\" for class_name in class_names]\n",
        "                  text_input = clip.tokenize(text_inputs).to(device)\n",
        "                  logits_per_image, logits_per_text = model(image, text_input)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu()*100  # Apply softmax\n",
        "\n",
        "    # Get the top 5 predicted classes\n",
        "    top5_probs, top5_classes = torch.topk(probs, 5, dim=1)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print the top 5 predicted classes and their probabilities\n",
        "    print(\"Top 5 predicted categories with CLIP:\")\n",
        "    for i in range(5):\n",
        "      print(f\"{class_names[top5_classes[0][i]]}: {top5_probs[0][i].item():.2f}%\")\n",
        "\n",
        "    rn_torch.eval()\n",
        "    with torch.no_grad():\n",
        "        logits_torch = rn_torch(img_t)\n",
        "\n",
        "\n",
        "    # Print top-5 ImageNet class names\n",
        "    import torch.nn.functional as F\n",
        "    probs_torch = F.softmax(logits_torch, dim=1)\n",
        "    # probs_clip = F.softmax(logits_clip, dim=1)\n",
        "\n",
        "    top5_torch = torch.topk(probs_torch, 5)\n",
        "    # top5_clip = torch.topk(probs_clip, 5)\n",
        "\n",
        "    print('\\nTop-5 ImageNet classes for ResNet-50 from torchvision:')\n",
        "    for idx in top5_torch.indices[0]:\n",
        "        print(f'{class_names[idx]}: {probs_torch[0, idx]:.2%}')\n",
        "\n",
        "    return prof\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prof=display_image_and_categories(img_path='4.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "RSJkh_sOHgXw",
        "outputId": "2db988b5-ed6c-49c7-e768-71f319ffbb6e"
      },
      "outputs": [],
      "source": [
        "display_image_and_categories('3.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "from torchvision import models\n",
        "import clip\n",
        "\n",
        "# Load the pre-trained CLIP RN50 image encoder (FP32)\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
        "\n",
        "model_half, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
        "# Convert the model to FP16\n",
        "model_half.visual = model.visual.half()  \n",
        "\n",
        "# Sample image for timing (replace with your actual image)\n",
        "image = preprocess(Image.open(\"4.jpg\")).unsqueeze(0).cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, image):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model.encode_image(image)  \n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Time the FP32 model (repeat for ~100 times)\n",
        "fp32_times = []\n",
        "for _ in range(100):\n",
        "    fp32_times.append(measure_inference_time(model.float(), image.clone()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Time the FP16 model (repeat for ~100 times)\n",
        "fp16_times = []\n",
        "for _ in range(100):\n",
        "    fp16_times.append(measure_inference_time(model, image.clone()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate and print results\n",
        "print(f\"FP32 Mean Inference Time: {torch.mean(torch.tensor(fp32_times))}\")\n",
        "print(f\"FP32 Inference Time Std Dev: {torch.std(torch.tensor(fp32_times))}\")\n",
        "\n",
        "print(f\"FP16 Mean Inference Time: {torch.mean(torch.tensor(fp16_times))}\")\n",
        "print(f\"FP16 Inference Time Std Dev: {torch.std(torch.tensor(fp16_times))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Difference in inference time\n",
        "print(f\"Mean Inference Time Speedup: {torch.mean(torch.tensor(fp32_times)) / torch.mean(torch.tensor(fp16_times))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get and compare probabilities\n",
        "def compare_probabilities(model, images):\n",
        "    with torch.no_grad():\n",
        "        # Calculate FP32 probabilities\n",
        "        fp32_logits = model.encode_image(torch.cat(images))\n",
        "        fp32_probabilities = fp32_logits.softmax(dim=-1)\n",
        "\n",
        "        # Calculate FP16 probabilities\n",
        "        model.half()  # Convert to FP16\n",
        "        fp16_logits = model.encode_image(torch.cat(images).half())\n",
        "        fp16_probabilities = fp16_logits.softmax(dim=-1)\n",
        "        model.float()  # Convert back to FP32 \n",
        "\n",
        "        # Print differences\n",
        "        for i in range(len(images)):\n",
        "            diff = torch.abs(fp32_probabilities[i] - fp16_probabilities[i])\n",
        "            print(f\"Max Probability Difference for Image {i}: {torch.max(diff)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = []\n",
        "path = 'Images/'\n",
        "for i in range(3, 6):\n",
        "    image = preprocess(Image.open(f\"{path}{i}.jpg\")).unsqueeze(0).cuda()\n",
        "    images.append(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "display_image_and_categories('Images/1.jpg')\n",
        "display_image_and_categories('Images/1.jpg',model_half)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "display_image_and_categories('Images/4.jpg')\n",
        "display_image_and_categories('Images/4.jpg',model_half)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "display_image_and_categories('Images/9.jpg')\n",
        "display_image_and_categories('Images/9.jpg',model_half)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a significant difference in the probability scores for the FP32 and FP16 models. The FP16 model has lower probability scores for the correct object category compared to the FP32 model. This difference in scores can be attributed to the reduced precision of the FP16 model, which might affect the model's ability to capture fine-grained details and make accurate predictions. However, since our prediction is almost always within the top-5 predictions, the model is still able to recognize the object category correctly despite the differences in probability scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prof1 = display_image_and_categories('Images/9.jpg')\n",
        "prof2= display_image_and_categories('Images/9.jpg',model_half)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(prof1.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(prof2.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prof1.export_chrome_trace(\"prof1.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
