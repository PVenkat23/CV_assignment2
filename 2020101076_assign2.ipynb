{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Assignment 2\n",
    "### Name: Pothugunta Venkat\n",
    "### Roll No: 2020101076"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "### Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.squeeze()) # changing shape from (1, 28, 28) to (28, 28)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implementing SIFT-BoVW-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT features shape: (469496, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "sift_features = []\n",
    "for images, labels in trainloader:\n",
    "    images = images.numpy() * 255  # Convert to numpy array and scale to 0-255\n",
    "    images = images.astype(np.uint8)  # Convert to uint8\n",
    "    for img in images:\n",
    "        kp, des = sift.detectAndCompute(img, None) # Compute SIFT features\n",
    "        if des is not None:\n",
    "            sift_features.append(des)\n",
    "\n",
    "sift_features = np.concatenate(sift_features, axis=0)\n",
    "print(\"SIFT features shape:\", sift_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=20, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KMeans<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KMeans(n_clusters=20, random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=20, random_state=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating bag of words\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "bag_of_words = KMeans(n_clusters=20, random_state=0, n_init=\"auto\")\n",
    "bag_of_words.fit(sift_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram for an image: [0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 3 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate histograms for given image\n",
    "def generate_histogram(img, bag_of_words):\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    if des is None:\n",
    "        return np.zeros(bag_of_words.n_clusters)\n",
    "    pred = bag_of_words.predict(des)\n",
    "    hist, _ = np.histogram(pred, bins=range(bag_of_words.n_clusters + 1))   \n",
    "    return hist\n",
    "\n",
    "print(\"Histogram for an image:\", generate_histogram(images[0], bag_of_words))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test set for SVM\n",
    "X_train = []\n",
    "y_train = []\n",
    "for images, labels in trainloader:\n",
    "    images = images.numpy() * 255\n",
    "    images = images.astype(np.uint8)\n",
    "    for img, label in zip(images, labels):\n",
    "        hist = generate_histogram(img, bag_of_words)\n",
    "        X_train.append(hist)\n",
    "        y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for images, labels in testloader:\n",
    "    images = images.numpy() * 255\n",
    "    images = images.astype(np.uint8)\n",
    "    for img, label in zip(images, labels):\n",
    "        hist = generate_histogram(img, bag_of_words)\n",
    "        X_test.append(hist)\n",
    "        y_test.append(label)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 20)\n",
      "y_train shape: (60000,)\n",
      "X_test shape: (10000, 20)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(gamma=&#x27;auto&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(gamma=&#x27;auto&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train SVM\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.71      0.64       980\n",
      "           1       0.84      0.97      0.90      1135\n",
      "           2       0.45      0.38      0.41      1032\n",
      "           3       0.47      0.46      0.46      1010\n",
      "           4       0.55      0.49      0.52       982\n",
      "           5       0.43      0.40      0.41       892\n",
      "           6       0.37      0.29      0.33       958\n",
      "           7       0.52      0.63      0.57      1028\n",
      "           8       0.61      0.55      0.58       974\n",
      "           9       0.50      0.53      0.51      1009\n",
      "\n",
      "    accuracy                           0.55     10000\n",
      "   macro avg       0.53      0.54      0.53     10000\n",
      "weighted avg       0.54      0.55      0.54     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Test SVM\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# report in a table\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sweeping number of clusters value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10 clusters: 0.4565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.71      0.58       980\n",
      "           1       0.84      0.97      0.90      1135\n",
      "           2       0.38      0.27      0.31      1032\n",
      "           3       0.34      0.36      0.35      1010\n",
      "           4       0.45      0.35      0.39       982\n",
      "           5       0.26      0.18      0.21       892\n",
      "           6       0.29      0.21      0.24       958\n",
      "           7       0.41      0.60      0.49      1028\n",
      "           8       0.48      0.36      0.41       974\n",
      "           9       0.40      0.45      0.42      1009\n",
      "\n",
      "    accuracy                           0.46     10000\n",
      "   macro avg       0.43      0.45      0.43     10000\n",
      "weighted avg       0.44      0.46      0.44     10000\n",
      "\n",
      "Accuracy for 20 clusters: 0.5479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.71      0.64       980\n",
      "           1       0.84      0.97      0.90      1135\n",
      "           2       0.45      0.39      0.41      1032\n",
      "           3       0.47      0.46      0.46      1010\n",
      "           4       0.55      0.49      0.52       982\n",
      "           5       0.43      0.40      0.41       892\n",
      "           6       0.37      0.29      0.33       958\n",
      "           7       0.52      0.63      0.57      1028\n",
      "           8       0.61      0.55      0.58       974\n",
      "           9       0.50      0.53      0.51      1009\n",
      "\n",
      "    accuracy                           0.55     10000\n",
      "   macro avg       0.53      0.54      0.53     10000\n",
      "weighted avg       0.54      0.55      0.54     10000\n",
      "\n",
      "Accuracy for 50 clusters: 0.6951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.78       980\n",
      "           1       0.83      0.98      0.90      1135\n",
      "           2       0.61      0.58      0.60      1032\n",
      "           3       0.73      0.59      0.66      1010\n",
      "           4       0.74      0.71      0.73       982\n",
      "           5       0.56      0.57      0.56       892\n",
      "           6       0.52      0.53      0.53       958\n",
      "           7       0.66      0.68      0.67      1028\n",
      "           8       0.85      0.76      0.80       974\n",
      "           9       0.69      0.64      0.66      1009\n",
      "\n",
      "    accuracy                           0.70     10000\n",
      "   macro avg       0.69      0.69      0.69     10000\n",
      "weighted avg       0.69      0.70      0.69     10000\n",
      "\n",
      "Accuracy for 100 clusters: 0.7494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.83       980\n",
      "           1       0.80      0.98      0.88      1135\n",
      "           2       0.65      0.61      0.63      1032\n",
      "           3       0.84      0.70      0.76      1010\n",
      "           4       0.83      0.80      0.82       982\n",
      "           5       0.67      0.67      0.67       892\n",
      "           6       0.63      0.62      0.62       958\n",
      "           7       0.70      0.70      0.70      1028\n",
      "           8       0.88      0.79      0.83       974\n",
      "           9       0.73      0.71      0.72      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "Accuracy for 200 clusters: 0.7847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       980\n",
      "           1       0.75      0.99      0.85      1135\n",
      "           2       0.74      0.68      0.71      1032\n",
      "           3       0.90      0.77      0.83      1010\n",
      "           4       0.88      0.82      0.85       982\n",
      "           5       0.74      0.66      0.70       892\n",
      "           6       0.67      0.68      0.67       958\n",
      "           7       0.70      0.72      0.71      1028\n",
      "           8       0.91      0.83      0.87       974\n",
      "           9       0.77      0.78      0.78      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.79      0.78      0.78     10000\n",
      "weighted avg       0.79      0.78      0.78     10000\n",
      "\n",
      "Accuracy for 500 clusters: 0.803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       980\n",
      "           1       0.71      0.98      0.82      1135\n",
      "           2       0.80      0.71      0.75      1032\n",
      "           3       0.94      0.80      0.87      1010\n",
      "           4       0.93      0.84      0.88       982\n",
      "           5       0.79      0.70      0.74       892\n",
      "           6       0.66      0.76      0.71       958\n",
      "           7       0.71      0.73      0.72      1028\n",
      "           8       0.94      0.84      0.89       974\n",
      "           9       0.82      0.75      0.78      1009\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.82      0.80      0.80     10000\n",
      "weighted avg       0.81      0.80      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_clusters = [10, 20, 50, 100, 200, 500]\n",
    "for n in num_of_clusters:\n",
    "    bag_of_words = KMeans(n_clusters=n, random_state=0, n_init=\"auto\")\n",
    "    bag_of_words.fit(sift_features)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for images, labels in trainloader:\n",
    "        images = images.numpy() * 255\n",
    "        images = images.astype(np.uint8)\n",
    "        for img, label in zip(images, labels):\n",
    "            hist = generate_histogram(img, bag_of_words)\n",
    "            X_train.append(hist)\n",
    "            y_train.append(label)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for images, labels in testloader:\n",
    "        images = images.numpy() * 255\n",
    "        images = images.astype(np.uint8)\n",
    "        for img, label in zip(images, labels):\n",
    "            hist = generate_histogram(img, bag_of_words)\n",
    "            X_test.append(hist)\n",
    "            y_test.append(label)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy for\", n, \"clusters:\", accuracy)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the accuracy significantly increases as we increase the cluster size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Results for 6 different hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT features shape: (624113, 128)\n",
      "Accuracy for {'nfeatures': 0, 'nOctaveLayers': 6, 'contrastThreshold': 0.04, 'edgeThreshold': 10.0, 'sigma': 1.6, 'contrastEnhanced': False, 'upright': False} : 0.6129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72       980\n",
      "           1       0.92      0.96      0.94      1135\n",
      "           2       0.48      0.43      0.45      1032\n",
      "           3       0.50      0.50      0.50      1010\n",
      "           4       0.61      0.63      0.62       982\n",
      "           5       0.50      0.46      0.48       892\n",
      "           6       0.48      0.40      0.43       958\n",
      "           7       0.59      0.66      0.62      1028\n",
      "           8       0.72      0.65      0.68       974\n",
      "           9       0.57      0.58      0.58      1009\n",
      "\n",
      "    accuracy                           0.61     10000\n",
      "   macro avg       0.60      0.61      0.60     10000\n",
      "weighted avg       0.61      0.61      0.61     10000\n",
      "\n",
      "SIFT features shape: (957357, 128)\n",
      "Accuracy for {'nfeatures': 0, 'nOctaveLayers': 10, 'contrastThreshold': 0.04, 'edgeThreshold': 10.0, 'sigma': 1.6, 'contrastEnhanced': False, 'upright': False} : 0.6387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       980\n",
      "           1       0.95      0.97      0.96      1135\n",
      "           2       0.50      0.45      0.47      1032\n",
      "           3       0.55      0.53      0.54      1010\n",
      "           4       0.63      0.70      0.66       982\n",
      "           5       0.46      0.46      0.46       892\n",
      "           6       0.54      0.45      0.49       958\n",
      "           7       0.60      0.67      0.63      1028\n",
      "           8       0.74      0.67      0.70       974\n",
      "           9       0.55      0.59      0.57      1009\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.63      0.63      0.63     10000\n",
      "weighted avg       0.63      0.64      0.64     10000\n",
      "\n",
      "SIFT features shape: (469436, 128)\n",
      "Accuracy for {'nfeatures': 0, 'nOctaveLayers': 3, 'contrastThreshold': 0.08, 'edgeThreshold': 10.0, 'sigma': 1.6, 'contrastEnhanced': False, 'upright': False} : 0.5776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69       980\n",
      "           1       0.86      0.97      0.91      1135\n",
      "           2       0.49      0.38      0.43      1032\n",
      "           3       0.54      0.50      0.52      1010\n",
      "           4       0.57      0.57      0.57       982\n",
      "           5       0.45      0.43      0.44       892\n",
      "           6       0.38      0.37      0.38       958\n",
      "           7       0.51      0.64      0.57      1028\n",
      "           8       0.68      0.57      0.62       974\n",
      "           9       0.58      0.50      0.54      1009\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.57      0.57      0.57     10000\n",
      "weighted avg       0.57      0.58      0.57     10000\n",
      "\n",
      "SIFT features shape: (464436, 128)\n",
      "Accuracy for {'nfeatures': 0, 'nOctaveLayers': 3, 'contrastThreshold': 0.16, 'edgeThreshold': 10.0, 'sigma': 1.6, 'contrastEnhanced': False, 'upright': False} : 0.5687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.78      0.69       980\n",
      "           1       0.85      0.97      0.91      1135\n",
      "           2       0.47      0.36      0.41      1032\n",
      "           3       0.49      0.47      0.48      1010\n",
      "           4       0.56      0.56      0.56       982\n",
      "           5       0.47      0.41      0.44       892\n",
      "           6       0.37      0.42      0.39       958\n",
      "           7       0.51      0.64      0.57      1028\n",
      "           8       0.68      0.55      0.61       974\n",
      "           9       0.58      0.47      0.52      1009\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.56      0.56      0.56     10000\n",
      "weighted avg       0.57      0.57      0.56     10000\n",
      "\n",
      "SIFT features shape: (336378, 128)\n",
      "Accuracy for {'nfeatures': 0, 'nOctaveLayers': 3, 'contrastThreshold': 0.04, 'edgeThreshold': 5.0, 'sigma': 1.6, 'contrastEnhanced': False, 'upright': False} : 0.5417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.72      0.55       980\n",
      "           1       0.78      0.97      0.87      1135\n",
      "           2       0.51      0.39      0.44      1032\n",
      "           3       0.56      0.46      0.50      1010\n",
      "           4       0.53      0.47      0.50       982\n",
      "           5       0.45      0.30      0.36       892\n",
      "           6       0.39      0.37      0.38       958\n",
      "           7       0.53      0.59      0.56      1028\n",
      "           8       0.60      0.54      0.57       974\n",
      "           9       0.53      0.52      0.52      1009\n",
      "\n",
      "    accuracy                           0.54     10000\n",
      "   macro avg       0.53      0.53      0.53     10000\n",
      "weighted avg       0.54      0.54      0.53     10000\n",
      "\n",
      "SIFT features shape: (224731, 128)\n",
      "Accuracy for {'nfeatures': 0, 'nOctaveLayers': 3, 'contrastThreshold': 0.04, 'edgeThreshold': 3.0, 'sigma': 1.6, 'contrastEnhanced': False, 'upright': False} : 0.4688\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.58      0.49       980\n",
      "           1       0.58      0.97      0.73      1135\n",
      "           2       0.37      0.35      0.36      1032\n",
      "           3       0.52      0.41      0.46      1010\n",
      "           4       0.51      0.31      0.39       982\n",
      "           5       0.49      0.29      0.37       892\n",
      "           6       0.30      0.19      0.23       958\n",
      "           7       0.40      0.49      0.44      1028\n",
      "           8       0.56      0.48      0.51       974\n",
      "           9       0.44      0.52      0.48      1009\n",
      "\n",
      "    accuracy                           0.47     10000\n",
      "   macro avg       0.46      0.46      0.45     10000\n",
      "weighted avg       0.46      0.47      0.45     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# changing parameters of sift\n",
    "\n",
    "default = {             ## These are the default parameters used in sift\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 3,\n",
    "    'contrastThreshold': 0.04,\n",
    "    'edgeThreshold': 10.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "params1 = {\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 6,\n",
    "    'contrastThreshold': 0.04,\n",
    "    'edgeThreshold': 10.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 10,\n",
    "    'contrastThreshold': 0.04,\n",
    "    'edgeThreshold': 10.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "params3 = {\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 3,\n",
    "    'contrastThreshold': 0.08,\n",
    "    'edgeThreshold': 10.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "params4 = {\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 3,\n",
    "    'contrastThreshold': 0.16,\n",
    "    'edgeThreshold': 10.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "params5 = {\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 3,\n",
    "    'contrastThreshold': 0.04,\n",
    "    'edgeThreshold': 5.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "params6 = {\n",
    "    'nfeatures': 0,\n",
    "    'nOctaveLayers': 3,\n",
    "    'contrastThreshold': 0.04,\n",
    "    'edgeThreshold': 3.0,\n",
    "    'sigma': 1.6,\n",
    "}\n",
    "\n",
    "# train and test sift features for different parameters\n",
    "params = [params1, params2, params3, params4, params5, params6]\n",
    "for param in params:\n",
    "    sift = cv2.SIFT_create()\n",
    "    sift.setNFeatures(param['nfeatures'])\n",
    "    sift.setNOctaveLayers(param['nOctaveLayers'])\n",
    "    sift.setContrastThreshold(param['contrastThreshold'])\n",
    "    sift.setEdgeThreshold(param['edgeThreshold'])\n",
    "    sift.setSigma(param['sigma'])\n",
    "    \n",
    "    sift_features = []\n",
    "    for images, labels in trainloader:\n",
    "        images = images.numpy() * 255\n",
    "        images = images.astype(np.uint8)\n",
    "        for img in images:\n",
    "            kp, des = sift.detectAndCompute(img, None)\n",
    "            if des is not None:\n",
    "                sift_features.append(des)\n",
    "\n",
    "    sift_features = np.concatenate(sift_features, axis=0)\n",
    "    print(\"SIFT features shape:\", sift_features.shape)\n",
    "\n",
    "    bag_of_words = KMeans(n_clusters=20, random_state=0, n_init=\"auto\")\n",
    "    bag_of_words.fit(sift_features)\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for images, labels in trainloader:\n",
    "        images = images.numpy() * 255\n",
    "        images = images.astype(np.uint8)\n",
    "        for img, label in zip(images, labels):\n",
    "            hist = generate_histogram(img, bag_of_words)\n",
    "            X_train.append(hist)\n",
    "            y_train.append(label)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for images, labels in testloader:\n",
    "        images = images.numpy() * 255\n",
    "        images = images.astype(np.uint8)\n",
    "        for img, label in zip(images, labels):\n",
    "            hist = generate_histogram(img, bag_of_words)\n",
    "            X_test.append(hist)\n",
    "            y_test.append(label)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(\"Accuracy for\", param, \":\", accuracy)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 CNNs and Transformers\n",
    "\n",
    "### 2.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "[1,   100] loss: 1.035\n",
      "[1,   200] loss: 0.258\n",
      "[1,   300] loss: 0.171\n",
      "[1,   400] loss: 0.141\n",
      "Epoch 1: Training Accuracy: 89.38 %\n",
      "[2,   100] loss: 0.104\n",
      "[2,   200] loss: 0.091\n",
      "[2,   300] loss: 0.094\n",
      "[2,   400] loss: 0.083\n",
      "Epoch 2: Training Accuracy: 97.19 %\n",
      "[3,   100] loss: 0.067\n",
      "[3,   200] loss: 0.069\n",
      "[3,   300] loss: 0.063\n",
      "[3,   400] loss: 0.064\n",
      "Epoch 3: Training Accuracy: 98.00 %\n",
      "[4,   100] loss: 0.049\n",
      "[4,   200] loss: 0.053\n",
      "[4,   300] loss: 0.053\n",
      "[4,   400] loss: 0.049\n",
      "Epoch 4: Training Accuracy: 98.43 %\n",
      "[5,   100] loss: 0.045\n",
      "[5,   200] loss: 0.046\n",
      "[5,   300] loss: 0.042\n",
      "[5,   400] loss: 0.042\n",
      "Epoch 5: Training Accuracy: 98.66 %\n",
      "[6,   100] loss: 0.036\n",
      "[6,   200] loss: 0.036\n",
      "[6,   300] loss: 0.035\n",
      "[6,   400] loss: 0.038\n",
      "Epoch 6: Training Accuracy: 98.86 %\n",
      "[7,   100] loss: 0.032\n",
      "[7,   200] loss: 0.036\n",
      "[7,   300] loss: 0.028\n",
      "[7,   400] loss: 0.032\n",
      "Epoch 7: Training Accuracy: 99.03 %\n",
      "[8,   100] loss: 0.027\n",
      "[8,   200] loss: 0.028\n",
      "[8,   300] loss: 0.027\n",
      "[8,   400] loss: 0.027\n",
      "Epoch 8: Training Accuracy: 99.12 %\n",
      "[9,   100] loss: 0.025\n",
      "[9,   200] loss: 0.022\n",
      "[9,   300] loss: 0.023\n",
      "[9,   400] loss: 0.025\n",
      "Epoch 9: Training Accuracy: 99.25 %\n",
      "[10,   100] loss: 0.019\n",
      "[10,   200] loss: 0.021\n",
      "[10,   300] loss: 0.018\n",
      "[10,   400] loss: 0.021\n",
      "Epoch 10: Training Accuracy: 99.36 %\n",
      "Test Accuracy: 99.07 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_data(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def train_model(model, trainloader, testloader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if i % 100 == 99:  \n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        print('Epoch %d: Training Accuracy: %.2f %%' % (epoch + 1, train_accuracy))\n",
    "\n",
    "def test_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print('Test Accuracy: %.2f %%' % (test_accuracy))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "\n",
    "    trainloader, testloader = load_data(batch_size=batch_size)\n",
    "    \n",
    "    model = LeNet()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, trainloader, testloader, criterion, optimizer, epochs, device)\n",
    "    test_model(model, testloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Setting 1:\n",
      "{'batch_size': 64, 'learning_rate': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "[1,   100] loss: 1.120\n",
      "[1,   200] loss: 0.316\n",
      "[1,   300] loss: 0.224\n",
      "[1,   400] loss: 0.185\n",
      "[1,   500] loss: 0.159\n",
      "[1,   600] loss: 0.141\n",
      "[1,   700] loss: 0.124\n",
      "[1,   800] loss: 0.108\n",
      "[1,   900] loss: 0.091\n",
      "Epoch 1: Training Accuracy: 91.89 %\n",
      "[2,   100] loss: 0.086\n",
      "[2,   200] loss: 0.089\n",
      "[2,   300] loss: 0.082\n",
      "[2,   400] loss: 0.067\n",
      "[2,   500] loss: 0.075\n",
      "[2,   600] loss: 0.078\n",
      "[2,   700] loss: 0.084\n",
      "[2,   800] loss: 0.066\n",
      "[2,   900] loss: 0.072\n",
      "Epoch 2: Training Accuracy: 97.64 %\n",
      "[3,   100] loss: 0.056\n",
      "[3,   200] loss: 0.059\n",
      "[3,   300] loss: 0.060\n",
      "[3,   400] loss: 0.050\n",
      "[3,   500] loss: 0.052\n",
      "[3,   600] loss: 0.062\n",
      "[3,   700] loss: 0.054\n",
      "[3,   800] loss: 0.047\n",
      "[3,   900] loss: 0.058\n",
      "Epoch 3: Training Accuracy: 98.34 %\n",
      "[4,   100] loss: 0.039\n",
      "[4,   200] loss: 0.052\n",
      "[4,   300] loss: 0.047\n",
      "[4,   400] loss: 0.040\n",
      "[4,   500] loss: 0.041\n",
      "[4,   600] loss: 0.044\n",
      "[4,   700] loss: 0.044\n",
      "[4,   800] loss: 0.042\n",
      "[4,   900] loss: 0.053\n",
      "Epoch 4: Training Accuracy: 98.60 %\n",
      "[5,   100] loss: 0.027\n",
      "[5,   200] loss: 0.033\n",
      "[5,   300] loss: 0.038\n",
      "[5,   400] loss: 0.031\n",
      "[5,   500] loss: 0.033\n",
      "[5,   600] loss: 0.042\n",
      "[5,   700] loss: 0.036\n",
      "[5,   800] loss: 0.041\n",
      "[5,   900] loss: 0.038\n",
      "Epoch 5: Training Accuracy: 98.83 %\n",
      "[6,   100] loss: 0.030\n",
      "[6,   200] loss: 0.035\n",
      "[6,   300] loss: 0.028\n",
      "[6,   400] loss: 0.023\n",
      "[6,   500] loss: 0.031\n",
      "[6,   600] loss: 0.029\n",
      "[6,   700] loss: 0.030\n",
      "[6,   800] loss: 0.033\n",
      "[6,   900] loss: 0.025\n",
      "Epoch 6: Training Accuracy: 99.06 %\n",
      "[7,   100] loss: 0.025\n",
      "[7,   200] loss: 0.024\n",
      "[7,   300] loss: 0.032\n",
      "[7,   400] loss: 0.026\n",
      "[7,   500] loss: 0.021\n",
      "[7,   600] loss: 0.033\n",
      "[7,   700] loss: 0.025\n",
      "[7,   800] loss: 0.024\n",
      "[7,   900] loss: 0.029\n",
      "Epoch 7: Training Accuracy: 99.14 %\n",
      "[8,   100] loss: 0.018\n",
      "[8,   200] loss: 0.025\n",
      "[8,   300] loss: 0.014\n",
      "[8,   400] loss: 0.019\n",
      "[8,   500] loss: 0.023\n",
      "[8,   600] loss: 0.030\n",
      "[8,   700] loss: 0.025\n",
      "[8,   800] loss: 0.022\n",
      "[8,   900] loss: 0.026\n",
      "Epoch 8: Training Accuracy: 99.30 %\n",
      "[9,   100] loss: 0.015\n",
      "[9,   200] loss: 0.012\n",
      "[9,   300] loss: 0.019\n",
      "[9,   400] loss: 0.021\n",
      "[9,   500] loss: 0.012\n",
      "[9,   600] loss: 0.018\n",
      "[9,   700] loss: 0.027\n",
      "[9,   800] loss: 0.021\n",
      "[9,   900] loss: 0.023\n",
      "Epoch 9: Training Accuracy: 99.42 %\n",
      "[10,   100] loss: 0.010\n",
      "[10,   200] loss: 0.016\n",
      "[10,   300] loss: 0.017\n",
      "[10,   400] loss: 0.021\n",
      "[10,   500] loss: 0.019\n",
      "[10,   600] loss: 0.022\n",
      "[10,   700] loss: 0.017\n",
      "[10,   800] loss: 0.020\n",
      "[10,   900] loss: 0.017\n",
      "Epoch 10: Training Accuracy: 99.39 %\n",
      "Test Accuracy: 99.04 %\n",
      "\n",
      "Setting 2:\n",
      "{'batch_size': 128, 'learning_rate': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "[1,   100] loss: 0.928\n",
      "[1,   200] loss: 0.223\n",
      "[1,   300] loss: 0.162\n",
      "[1,   400] loss: 0.130\n",
      "Epoch 1: Training Accuracy: 90.69 %\n",
      "[2,   100] loss: 0.096\n",
      "[2,   200] loss: 0.090\n",
      "[2,   300] loss: 0.085\n",
      "[2,   400] loss: 0.073\n",
      "Epoch 2: Training Accuracy: 97.41 %\n",
      "[3,   100] loss: 0.064\n",
      "[3,   200] loss: 0.067\n",
      "[3,   300] loss: 0.063\n",
      "[3,   400] loss: 0.057\n",
      "Epoch 3: Training Accuracy: 98.03 %\n",
      "[4,   100] loss: 0.052\n",
      "[4,   200] loss: 0.047\n",
      "[4,   300] loss: 0.054\n",
      "[4,   400] loss: 0.045\n",
      "Epoch 4: Training Accuracy: 98.45 %\n",
      "[5,   100] loss: 0.042\n",
      "[5,   200] loss: 0.037\n",
      "[5,   300] loss: 0.038\n",
      "[5,   400] loss: 0.039\n",
      "Epoch 5: Training Accuracy: 98.79 %\n",
      "[6,   100] loss: 0.027\n",
      "[6,   200] loss: 0.035\n",
      "[6,   300] loss: 0.037\n",
      "[6,   400] loss: 0.036\n",
      "Epoch 6: Training Accuracy: 98.93 %\n",
      "[7,   100] loss: 0.027\n",
      "[7,   200] loss: 0.032\n",
      "[7,   300] loss: 0.029\n",
      "[7,   400] loss: 0.030\n",
      "Epoch 7: Training Accuracy: 99.06 %\n",
      "[8,   100] loss: 0.022\n",
      "[8,   200] loss: 0.023\n",
      "[8,   300] loss: 0.023\n",
      "[8,   400] loss: 0.031\n",
      "Epoch 8: Training Accuracy: 99.22 %\n",
      "[9,   100] loss: 0.018\n",
      "[9,   200] loss: 0.018\n",
      "[9,   300] loss: 0.021\n",
      "[9,   400] loss: 0.024\n",
      "Epoch 9: Training Accuracy: 99.31 %\n",
      "[10,   100] loss: 0.016\n",
      "[10,   200] loss: 0.014\n",
      "[10,   300] loss: 0.019\n",
      "[10,   400] loss: 0.023\n",
      "Epoch 10: Training Accuracy: 99.40 %\n",
      "Test Accuracy: 99.05 %\n",
      "\n",
      "Setting 3:\n",
      "{'batch_size': 128, 'learning_rate': 0.0005, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "[1,   100] loss: 1.350\n",
      "[1,   200] loss: 0.350\n",
      "[1,   300] loss: 0.247\n",
      "[1,   400] loss: 0.207\n",
      "Epoch 1: Training Accuracy: 85.87 %\n",
      "[2,   100] loss: 0.142\n",
      "[2,   200] loss: 0.142\n",
      "[2,   300] loss: 0.119\n",
      "[2,   400] loss: 0.112\n",
      "Epoch 2: Training Accuracy: 96.12 %\n",
      "[3,   100] loss: 0.095\n",
      "[3,   200] loss: 0.087\n",
      "[3,   300] loss: 0.094\n",
      "[3,   400] loss: 0.086\n",
      "Epoch 3: Training Accuracy: 97.22 %\n",
      "[4,   100] loss: 0.077\n",
      "[4,   200] loss: 0.072\n",
      "[4,   300] loss: 0.073\n",
      "[4,   400] loss: 0.068\n",
      "Epoch 4: Training Accuracy: 97.80 %\n",
      "[5,   100] loss: 0.059\n",
      "[5,   200] loss: 0.055\n",
      "[5,   300] loss: 0.053\n",
      "[5,   400] loss: 0.057\n",
      "Epoch 5: Training Accuracy: 98.16 %\n",
      "[6,   100] loss: 0.043\n",
      "[6,   200] loss: 0.055\n",
      "[6,   300] loss: 0.050\n",
      "[6,   400] loss: 0.052\n",
      "Epoch 6: Training Accuracy: 98.47 %\n",
      "[7,   100] loss: 0.043\n",
      "[7,   200] loss: 0.044\n",
      "[7,   300] loss: 0.044\n",
      "[7,   400] loss: 0.044\n",
      "Epoch 7: Training Accuracy: 98.66 %\n",
      "[8,   100] loss: 0.031\n",
      "[8,   200] loss: 0.040\n",
      "[8,   300] loss: 0.037\n",
      "[8,   400] loss: 0.039\n",
      "Epoch 8: Training Accuracy: 98.84 %\n",
      "[9,   100] loss: 0.036\n",
      "[9,   200] loss: 0.034\n",
      "[9,   300] loss: 0.027\n",
      "[9,   400] loss: 0.037\n",
      "Epoch 9: Training Accuracy: 98.97 %\n",
      "[10,   100] loss: 0.030\n",
      "[10,   200] loss: 0.028\n",
      "[10,   300] loss: 0.033\n",
      "[10,   400] loss: 0.032\n",
      "Epoch 10: Training Accuracy: 99.05 %\n",
      "Test Accuracy: 98.81 %\n",
      "\n",
      "Setting 4:\n",
      "{'batch_size': 128, 'learning_rate': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "[1,   100] loss: 2.301\n",
      "[1,   200] loss: 2.302\n",
      "[1,   300] loss: 2.300\n",
      "[1,   400] loss: 2.300\n",
      "Epoch 1: Training Accuracy: 12.65 %\n",
      "[2,   100] loss: 2.297\n",
      "[2,   200] loss: 2.297\n",
      "[2,   300] loss: 2.296\n",
      "[2,   400] loss: 2.296\n",
      "Epoch 2: Training Accuracy: 18.58 %\n",
      "[3,   100] loss: 2.294\n",
      "[3,   200] loss: 2.293\n",
      "[3,   300] loss: 2.291\n",
      "[3,   400] loss: 2.292\n",
      "Epoch 3: Training Accuracy: 23.96 %\n",
      "[4,   100] loss: 2.290\n",
      "[4,   200] loss: 2.288\n",
      "[4,   300] loss: 2.287\n",
      "[4,   400] loss: 2.286\n",
      "Epoch 4: Training Accuracy: 26.67 %\n",
      "[5,   100] loss: 2.285\n",
      "[5,   200] loss: 2.285\n",
      "[5,   300] loss: 2.282\n",
      "[5,   400] loss: 2.280\n",
      "Epoch 5: Training Accuracy: 29.01 %\n",
      "[6,   100] loss: 2.278\n",
      "[6,   200] loss: 2.277\n",
      "[6,   300] loss: 2.276\n",
      "[6,   400] loss: 2.274\n",
      "Epoch 6: Training Accuracy: 31.85 %\n",
      "[7,   100] loss: 2.271\n",
      "[7,   200] loss: 2.269\n",
      "[7,   300] loss: 2.266\n",
      "[7,   400] loss: 2.264\n",
      "Epoch 7: Training Accuracy: 35.90 %\n",
      "[8,   100] loss: 2.260\n",
      "[8,   200] loss: 2.258\n",
      "[8,   300] loss: 2.253\n",
      "[8,   400] loss: 2.253\n",
      "Epoch 8: Training Accuracy: 39.43 %\n",
      "[9,   100] loss: 2.247\n",
      "[9,   200] loss: 2.241\n",
      "[9,   300] loss: 2.237\n",
      "[9,   400] loss: 2.231\n",
      "Epoch 9: Training Accuracy: 42.03 %\n",
      "[10,   100] loss: 2.222\n",
      "[10,   200] loss: 2.216\n",
      "[10,   300] loss: 2.208\n",
      "[10,   400] loss: 2.201\n",
      "Epoch 10: Training Accuracy: 44.55 %\n",
      "Test Accuracy: 46.84 %\n",
      "\n",
      "Setting 5:\n",
      "{'batch_size': 128, 'learning_rate': 0.001, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "[1,   100] loss: 0.587\n",
      "[1,   200] loss: 0.152\n",
      "[1,   300] loss: 0.116\n",
      "[1,   400] loss: 0.091\n",
      "Epoch 1: Training Accuracy: 93.19 %\n",
      "[2,   100] loss: 0.073\n",
      "[2,   200] loss: 0.072\n",
      "[2,   300] loss: 0.060\n",
      "[2,   400] loss: 0.062\n",
      "Epoch 2: Training Accuracy: 97.96 %\n",
      "[3,   100] loss: 0.047\n",
      "[3,   200] loss: 0.053\n",
      "[3,   300] loss: 0.044\n",
      "[3,   400] loss: 0.046\n",
      "Epoch 3: Training Accuracy: 98.50 %\n",
      "[4,   100] loss: 0.040\n",
      "[4,   200] loss: 0.037\n",
      "[4,   300] loss: 0.035\n",
      "[4,   400] loss: 0.037\n",
      "Epoch 4: Training Accuracy: 98.83 %\n",
      "[5,   100] loss: 0.030\n",
      "[5,   200] loss: 0.030\n",
      "[5,   300] loss: 0.030\n",
      "[5,   400] loss: 0.030\n",
      "Epoch 5: Training Accuracy: 99.06 %\n",
      "[6,   100] loss: 0.019\n",
      "[6,   200] loss: 0.026\n",
      "[6,   300] loss: 0.025\n",
      "[6,   400] loss: 0.024\n",
      "Epoch 6: Training Accuracy: 99.22 %\n",
      "[7,   100] loss: 0.022\n",
      "[7,   200] loss: 0.021\n",
      "[7,   300] loss: 0.017\n",
      "[7,   400] loss: 0.020\n",
      "Epoch 7: Training Accuracy: 99.32 %\n",
      "[8,   100] loss: 0.015\n",
      "[8,   200] loss: 0.015\n",
      "[8,   300] loss: 0.018\n",
      "[8,   400] loss: 0.020\n",
      "Epoch 8: Training Accuracy: 99.46 %\n",
      "[9,   100] loss: 0.012\n",
      "[9,   200] loss: 0.012\n",
      "[9,   300] loss: 0.018\n",
      "[9,   400] loss: 0.017\n",
      "Epoch 9: Training Accuracy: 99.53 %\n",
      "[10,   100] loss: 0.011\n",
      "[10,   200] loss: 0.011\n",
      "[10,   300] loss: 0.017\n",
      "[10,   400] loss: 0.013\n",
      "Epoch 10: Training Accuracy: 99.59 %\n",
      "Test Accuracy: 99.22 %\n",
      "\n",
      "Setting 6:\n",
      "{'batch_size': 256, 'learning_rate': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "[1,   100] loss: 0.857\n",
      "[1,   200] loss: 0.201\n",
      "Epoch 1: Training Accuracy: 86.43 %\n",
      "[2,   100] loss: 0.119\n",
      "[2,   200] loss: 0.095\n",
      "Epoch 2: Training Accuracy: 96.84 %\n",
      "[3,   100] loss: 0.078\n",
      "[3,   200] loss: 0.071\n",
      "Epoch 3: Training Accuracy: 97.74 %\n",
      "[4,   100] loss: 0.060\n",
      "[4,   200] loss: 0.060\n",
      "Epoch 4: Training Accuracy: 98.16 %\n",
      "[5,   100] loss: 0.049\n",
      "[5,   200] loss: 0.053\n",
      "Epoch 5: Training Accuracy: 98.38 %\n",
      "[6,   100] loss: 0.041\n",
      "[6,   200] loss: 0.047\n",
      "Epoch 6: Training Accuracy: 98.63 %\n",
      "[7,   100] loss: 0.038\n",
      "[7,   200] loss: 0.038\n",
      "Epoch 7: Training Accuracy: 98.78 %\n",
      "[8,   100] loss: 0.034\n",
      "[8,   200] loss: 0.032\n",
      "Epoch 8: Training Accuracy: 98.94 %\n",
      "[9,   100] loss: 0.027\n",
      "[9,   200] loss: 0.032\n",
      "Epoch 9: Training Accuracy: 99.05 %\n",
      "[10,   100] loss: 0.025\n",
      "[10,   200] loss: 0.024\n",
      "Epoch 10: Training Accuracy: 99.19 %\n",
      "Test Accuracy: 98.43 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# six different parameters for LeNet\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Different hyperparameters settings\n",
    "    hyperparameters = [\n",
    "        {\"batch_size\": 64, \"learning_rate\": 0.001, \"optimizer\": optim.Adam},\n",
    "        {\"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": optim.Adam},\n",
    "        {\"batch_size\": 128, \"learning_rate\": 0.0005, \"optimizer\": optim.Adam},\n",
    "        {\"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": optim.SGD},\n",
    "        {\"batch_size\": 128, \"learning_rate\": 0.001, \"optimizer\": optim.RMSprop},\n",
    "        {\"batch_size\": 256, \"learning_rate\": 0.001, \"optimizer\": optim.Adam}\n",
    "    ]\n",
    "\n",
    "    for i, params in enumerate(hyperparameters, 1):\n",
    "        print(f\"Setting {i}:\")\n",
    "        print(params)\n",
    "\n",
    "        # Load data\n",
    "        trainloader, testloader = load_data(batch_size=params[\"batch_size\"])\n",
    "\n",
    "        # Initialize model, criterion, and optimizer\n",
    "        model = LeNet()\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = params[\"optimizer\"](model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "        # Train the model\n",
    "        train_model(model, trainloader, testloader, criterion, optimizer, epochs=10, device=device)\n",
    "        \n",
    "        # Test the model\n",
    "        test_model(model, testloader, device)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 best CNN vs SIFT-BoVW-SVM\n",
    "CNN test accuracy: 99.22 %\n",
    "\n",
    "SIFT-BoVW-SVM test accuracy: 80.0%\n",
    "\n",
    "CNNs learn features directly from raw pixel values, capturing both low-level and high-level representations. In contrast, SIFT extracts handcrafted features and quantizes them into visual words, potentially missing out on relevant information.\n",
    "\n",
    "CNNs generally outperform traditional methods like SIFT with BoW and SVM on image classification tasks, especially when dealing with large datasets like MNIST. They can capture more complex patterns and variations in the data, leading to higher accuracy rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetModified(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNetModified, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "[1,   100] loss: 1.030\n",
      "[1,   200] loss: 0.257\n",
      "[1,   300] loss: 0.162\n",
      "[1,   400] loss: 0.133\n",
      "Epoch 1: Training Accuracy: 89.68 %\n",
      "[2,   100] loss: 0.096\n",
      "[2,   200] loss: 0.094\n",
      "[2,   300] loss: 0.087\n",
      "[2,   400] loss: 0.086\n",
      "Epoch 2: Training Accuracy: 97.29 %\n",
      "[3,   100] loss: 0.064\n",
      "[3,   200] loss: 0.066\n",
      "[3,   300] loss: 0.059\n",
      "[3,   400] loss: 0.065\n",
      "Epoch 3: Training Accuracy: 98.03 %\n",
      "[4,   100] loss: 0.051\n",
      "[4,   200] loss: 0.055\n",
      "[4,   300] loss: 0.046\n",
      "[4,   400] loss: 0.047\n",
      "Epoch 4: Training Accuracy: 98.47 %\n",
      "[5,   100] loss: 0.039\n",
      "[5,   200] loss: 0.040\n",
      "[5,   300] loss: 0.044\n",
      "[5,   400] loss: 0.037\n",
      "Epoch 5: Training Accuracy: 98.72 %\n",
      "[6,   100] loss: 0.034\n",
      "[6,   200] loss: 0.034\n",
      "[6,   300] loss: 0.037\n",
      "[6,   400] loss: 0.033\n",
      "Epoch 6: Training Accuracy: 98.89 %\n",
      "[7,   100] loss: 0.027\n",
      "[7,   200] loss: 0.027\n",
      "[7,   300] loss: 0.030\n",
      "[7,   400] loss: 0.028\n",
      "Epoch 7: Training Accuracy: 99.05 %\n",
      "[8,   100] loss: 0.025\n",
      "[8,   200] loss: 0.025\n",
      "[8,   300] loss: 0.024\n",
      "[8,   400] loss: 0.030\n",
      "Epoch 8: Training Accuracy: 99.16 %\n",
      "[9,   100] loss: 0.019\n",
      "[9,   200] loss: 0.020\n",
      "[9,   300] loss: 0.026\n",
      "[9,   400] loss: 0.029\n",
      "Epoch 9: Training Accuracy: 99.21 %\n",
      "[10,   100] loss: 0.016\n",
      "[10,   200] loss: 0.017\n",
      "[10,   300] loss: 0.023\n",
      "[10,   400] loss: 0.023\n",
      "Epoch 10: Training Accuracy: 99.36 %\n",
      "Test Accuracy: 98.97 %\n"
     ]
    }
   ],
   "source": [
    "def load_data(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def train_model(model, trainloader, testloader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if i % 100 == 99:  \n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        print('Epoch %d: Training Accuracy: %.2f %%' % (epoch + 1, train_accuracy))\n",
    "\n",
    "def test_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print('Test Accuracy: %.2f %%' % (test_accuracy))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "\n",
    "    trainloader, testloader = load_data(batch_size=batch_size)\n",
    "    \n",
    "    model = LeNetModified()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, trainloader, testloader, criterion, optimizer, epochs, device)\n",
    "    test_model(model, testloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doubling the number of convolution has many benefits as it can learn more complex function but it also has it's disadvantages due to Overfitting of Vanishing or Exlploding gradients problems.\n",
    "\n",
    "In my case the test accuracy is sligthy reduced comapared to before.\n",
    "\n",
    "### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 600 samples:\n",
      "Epoch 1, Loss: 2.294\n",
      "Epoch 2, Loss: 2.252\n",
      "Epoch 3, Loss: 2.186\n",
      "Epoch 4, Loss: 2.060\n",
      "Epoch 5, Loss: 1.888\n",
      "Accuracy on test set: 42.76 %\n",
      "\n",
      "Training with 1800 samples:\n",
      "Epoch 1, Loss: 2.227\n",
      "Epoch 2, Loss: 1.738\n",
      "Epoch 3, Loss: 0.897\n",
      "Epoch 4, Loss: 0.538\n",
      "Epoch 5, Loss: 0.440\n",
      "Accuracy on test set: 88.92 %\n",
      "\n",
      "Training with 6000 samples:\n",
      "Epoch 1, Loss: 1.650\n",
      "Epoch 2, Loss: 0.523\n",
      "Epoch 3, Loss: 0.331\n",
      "Epoch 4, Loss: 0.242\n",
      "Epoch 5, Loss: 0.192\n",
      "Accuracy on test set: 94.10 %\n",
      "\n",
      "Training with 18000 samples:\n",
      "Epoch 1, Loss: 0.824\n",
      "Epoch 2, Loss: 0.187\n",
      "Epoch 3, Loss: 0.117\n",
      "Epoch 4, Loss: 0.091\n",
      "Epoch 5, Loss: 0.074\n",
      "Accuracy on test set: 97.70 %\n",
      "\n",
      "Training with 60000 samples:\n",
      "Epoch 1, Loss: 0.340\n",
      "Epoch 2, Loss: 0.081\n",
      "Epoch 3, Loss: 0.061\n",
      "Epoch 4, Loss: 0.048\n",
      "Epoch 5, Loss: 0.040\n",
      "Accuracy on test set: 98.86 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqgUlEQVR4nO3deXhM59sH8O9MlskmiyUbkYTEvicaQcuvYm9eWm1srVjaqqWqSlstIlQtbdGiqGpsVUuLUsReSolQlFK1xFIkKbKKTCYzz/tHzJEx2SZmifH9XFcumeecOXPPnUnO7T7POUcmhBAgIiIislJySwdAREREZEosdoiIiMiqsdghIiIiq8Zih4iIiKwaix0iIiKyaix2iIiIyKqx2CEiIiKrxmKHiIiIrBqLHSIiIrJqLHbIIj777DPUqlULNjY2aNasmaXDoSfUwIED4eLiYukwyiw+Ph7NmjWDg4MDZDIZ0tPTLRbLwIEDERAQUK7nTp48GTKZzLgBPQV+/fVXyGQy/Prrr5YO5anDYocAAMuWLYNMJpO+HBwcUKdOHYwcORIpKSlGfa2dO3fi/fffR5s2bRAXF4dPP/3UqNsn42rfvj1kMhkiIyP1ll25cgUymQyff/65BSJ7sty5cwdRUVFwdHTEggULsHLlSjg7O+utV/j3sKSvp3mHuWXLFrRr1w6enp5wcnJCrVq1EBUVhfj4eEuHRhWUraUDoIplypQpCAwMRG5uLg4ePIiFCxdi27ZtOHPmDJycnIzyGnv37oVcLsfSpUthb29vlG2S6f3yyy84fvw4QkJCLB3KEykxMRFZWVmYOnUqIiIiil1v5cqVOo9XrFiBXbt26Y3Xr1//seJZsmQJNBpNuZ47YcIEfPjhh4/1+uX1+eefY9y4cWjXrh3Gjx8PJycnXLx4Ebt378aaNWvQpUsXi8RFFRuLHdLRtWtXhIaGAgBef/11VKlSBbNnz8bPP/+Mvn37Pta2c3Jy4OTkhNTUVDg6Ohqt0BFCIDc3F46OjkbZHumrWbMmsrKyEBsbi82bN1s6HLMy1ucrNTUVAODu7l7ieq+++qrO4yNHjmDXrl1644/S/n6VlZ2dXZnXfZStrS1sbc2/+8jPz8fUqVPRsWNH7Ny5U2+5NsdEj+JhLCrR888/DwBISkqSxlatWoWQkBA4OjqicuXK6NOnD65fv67zvPbt26NRo0Y4fvw4nnvuOTg5OeGjjz6CTCZDXFwc7t27J7Xjly1bBuDhH7LatWtDoVAgICAAH330EZRKpc62AwIC8MILL2DHjh0IDQ2Fo6MjFi9eLB0PX7duHWJjY1G9enVUqlQJL7/8MjIyMqBUKjF69Gh4enrCxcUFgwYN0tt2XFwcnn/+eXh6ekKhUKBBgwZYuHChXl60MRw8eBDPPPMMHBwcUKtWLaxYsUJv3fT0dLz77rsICAiAQqFAjRo1MGDAANy+fVtaR6lUIiYmBkFBQVAoFPDz88P777+vF9+jRo4cCRcXF+Tk5Ogt69u3L7y9vaFWqwEAx44dQ+fOnVG1alU4OjoiMDAQgwcPLnH7WpUqVcK7776LLVu24I8//ihx3eLmc2gPlV65ckUa0+bx119/lX6WjRs3lg7RbNiwAY0bN4aDgwNCQkJw4sSJIl/z8uXL6Ny5M5ydneHr64spU6ZACKGzjkajwdy5c9GwYUM4ODjAy8sLQ4cORVpams56xX2+SrJ+/Xrpd6Jq1ap49dVXcePGDWl5+/btER0dDQBo2bIlZDIZBg4cWOI2S1Lc7xcA/Pzzz+jevTt8fX2hUChQu3ZtTJ06VfocaD06Z6fwIclvvvlG+j1s2bIlEhMTdZ5b1M9YJpNh5MiR2LRpExo1agSFQoGGDRsWeWhJ+/N2cHBA7dq1sXjx4jLNA7p9+zYyMzPRpk2bIpd7enpK3+fl5WHSpEkICQmBm5sbnJ2d8eyzz2Lfvn06zyn8vhcsWIBatWrByckJnTp1wvXr1yGEwNSpU1GjRg04OjqiR48euHv3rs42tJ+ZnTt3SnOyGjRogA0bNpT4frQSEhLQpUsXuLm5wcnJCe3atcOhQ4d01snKysLo0aOlvyOenp7o2LFjqb+P9IAgEkLExcUJACIxMVFn/MsvvxQAxKJFi4QQQnzyySdCJpOJ3r17i6+//lrExsaKqlWrioCAAJGWliY9r127dsLb21tUq1ZNvP3222Lx4sVi06ZNYuXKleLZZ58VCoVCrFy5UqxcuVJcunRJCCFEdHS0ACBefvllsWDBAjFgwAABQPTs2VMnJn9/fxEUFCQ8PDzEhx9+KBYtWiT27dsn9u3bJwCIZs2aifDwcPHVV1+JUaNGCZlMJvr06SP69esnunbtKhYsWCBee+01AUDExsbqbLtly5Zi4MCBYs6cOWLevHmiU6dOAoCYP3++Xgx169YVXl5e4qOPPhLz588XLVq0EDKZTJw5c0ZaLysrSzRq1EjY2NiIN954QyxcuFBMnTpVtGzZUpw4cUIIIYRarRadOnUSTk5OYvTo0WLx4sVi5MiRwtbWVvTo0aPEn9uBAwcEALFu3Tqd8Xv37glnZ2cxYsQIIYQQKSkpwsPDQ9SpU0d89tlnYsmSJeLjjz8W9evXL3H72p9lw4YNRUZGhvDw8BCRkZHSsqSkJAFAfPbZZ9JYTEyMKOpPi/YzlpSUpJdHHx8fMXnyZDFnzhxRvXp14eLiIlatWiVq1qwpZsyYIWbMmCHc3NxEUFCQUKvV0vOjo6OFg4ODCA4OFq+99pqYP3++eOGFFwQAMXHiRJ3Xf/3114Wtra144403xKJFi8QHH3wgnJ2dRcuWLUVeXp5OTEV9voqjfV8tW7YUc+bMER9++KFwdHTU+Z3YuXOnePPNNwUAMWXKFLFy5Urx+++/l5p7IYQYMWKEXj6L+/0SQoiePXuKqKgo8dlnn4mFCxeKV155RQAQY8eO1dlGdHS08Pf3lx5rf5bNmzcXQUFBYubMmWLWrFmiatWqokaNGjo5KupnDEA0bdpU+Pj4iKlTp4q5c+eKWrVqCScnJ3H79m1pvT/++EMoFAoREBAgZsyYIaZNmyZ8fX1F06ZNi/zcFKZWq4Wjo6MICQkRd+7cKXHd//77T/j4+IgxY8aIhQsXilmzZom6desKOzs76Xev8Ptu1qyZaNCggZg9e7aYMGGCsLe3F61atRIfffSRaN26tc7fk0GDBum8lr+/v6hTp45wd3cXH374oZg9e7Zo3LixkMvlYufOndJ62r9RhT9Pe/bsEfb29iI8PFx88cUXYs6cOaJJkybC3t5eJCQkSOv169dP2NvbizFjxohvv/1WzJw5U0RGRopVq1aVmAcqwGKHhBAP/2Dv3r1b/Pfff+L69etizZo1okqVKsLR0VH8+++/4sqVK8LGxkZMmzZN57mnT58Wtra2OuPt2rXTKZIKi46OFs7OzjpjJ0+eFADE66+/rjM+duxYAUDs3btXGvP39xcARHx8vM662j8kjRo10vnD3LdvXyGTyUTXrl111g8PD9f5Yy+EEDk5OXrxdu7cWdSqVUtnTBvDgQMHpLHU1FShUCjEe++9J41NmjRJABAbNmzQ265GoxFCCLFy5Uohl8vFb7/9prN80aJFAoA4dOiQ3nMLb6N69eqiV69eOuPr1q3TiW/jxo1FFrNloS12hBAiNjZWABDHjx8XQhin2AGgs+PfsWOHACAcHR3F1atXpfHFixfr7Si0BfLbb7+tk5Pu3bsLe3t78d9//wkhhPjtt98EAPH999/rxBQfH683Xtznqyh5eXnC09NTNGrUSNy/f18a/+WXXwQAMWnSJL33b+jPoLhip7jfr6I+w0OHDhVOTk4iNzdXGiuu2KlSpYq4e/euNP7zzz8LAGLLli3SWHHFjr29vbh48aI0durUKQFAzJs3TxqLjIwUTk5O4saNG9LYhQsXhK2tbanFjhAPf6ecnZ1F165dxbRp06TPY2H5+flCqVTqjKWlpQkvLy8xePBgvfddrVo1kZ6eLo2PHz9eKuBUKpU03rdvX2Fvb6+TS+1n5qeffpLGMjIyhI+Pj2jevLk09mixo9FoRHBwsOjcubP090CIgp9hYGCg6NixozTm5uYm/eeFDMfDWKQjIiIC1apVg5+fH/r06QMXFxds3LgR1atXx4YNG6DRaBAVFYXbt29LX97e3ggODtZrDysUCgwaNKhMr7tt2zYAwJgxY3TG33vvPQDA1q1bdcYDAwPRuXPnIrc1YMAAnfkIYWFhEELoHbIJCwvD9evXkZ+fL40VnpeRkZGB27dvo127drh8+TIyMjJ0nt+gQQM8++yz0uNq1aqhbt26uHz5sjT2008/oWnTpnjxxRf14tS27NevX4/69eujXr16OnnVHkJ8NK+PbuOVV17Btm3bkJ2dLY2vXbsW1atXR9u2bQE8nCfyyy+/QKVSFbu90rzzzjvw8PBAbGxsubfxqAYNGiA8PFx6HBYWBqDgEGrNmjX1xgvnV2vkyJHS99rDKXl5edi9ezeAghy7ubmhY8eOOjkOCQmBi4uLXo5L+nwVduzYMaSmpmL48OFwcHCQxrt374569erpfW6Nqbjfr8Kf4aysLNy+fRvPPvsscnJy8Pfff5e63d69e8PDw0N6rP2MF5X3R0VERKB27drS4yZNmsDV1VV6rlqtxu7du9GzZ0/4+vpK6wUFBaFr166lbh8AYmNjsXr1ajRv3hw7duzAxx9/jJCQELRo0QLnzp2T1rOxsZHmBWo0Gty9exf5+fkIDQ0t8tDPK6+8Ajc3N+mx9vP26quv6sxPCgsLQ15ens5hSgDw9fXV+T13dXXFgAEDcOLECSQnJxf5Xk6ePIkLFy6gX79+uHPnjvS5vHfvHjp06IADBw5Ik8jd3d2RkJCAmzdvlilPpIsTlEnHggULUKdOHdja2sLLywt169aFXF5QE1+4cAFCCAQHBxf53EcnPFavXr3Mk5CvXr0KuVyOoKAgnXFvb2+4u7vj6tWrOuOBgYHFbqvwDhKA9AfMz89Pb1yj0SAjIwNVqlQBABw6dAgxMTE4fPiw3jyYjIwMnT+Gj74OAHh4eOjMAbl06RJ69epVbKxAQV7PnTuHatWqFbm8tEmXvXv3xty5c7F582b069cP2dnZ2LZtG4YOHSoVVO3atUOvXr0QGxuLOXPmoH379ujZsyf69esHhUJR4vYLc3Nzw+jRoxETE4MTJ07o7BTLy5CfFwC9OTZyuRy1atXSGatTpw4ASPODLly4gIyMDJ05HYU9muOSPl+FaT+XdevW1VtWr149HDx4sEzbKY/ifr/++usvTJgwAXv37kVmZqbOskcL9qI8+vPQ/owfzXtZnqt9vva5qampuH//vt7vOYAix4rTt29f9O3bF5mZmUhISMCyZcuwevVqREZG4syZM1LhuXz5cnzxxRf4+++/dYr8on6+j/s5DAoK0ptzVPhz6O3trfeaFy5cAABpPldRMjIy4OHhgVmzZiE6Ohp+fn4ICQlBt27dMGDAAL3PPhWNxQ7peOaZZ6SzsR6l0Wggk8mwfft22NjY6C1/9OJu5Tl7pawXKitp20XFVtK4eDCR9dKlS+jQoQPq1auH2bNnw8/PD/b29ti2bRvmzJmjd5puadsrK41Gg8aNG2P27NlFLn/0j+2jWrVqhYCAAKxbtw79+vXDli1bcP/+ffTu3VtaRyaT4ccff8SRI0ewZcsW7NixA4MHD8YXX3yBI0eOGHRhvnfeeQdz5sxBbGws5s6dq7e8uJ/hoxNktcr78zKERqOBp6cnvv/++yKXP1poPgln9hUVY3p6Otq1awdXV1dMmTIFtWvXhoODA/744w988MEHZTrV/HHybsyfWVm4urqiY8eO6NixI+zs7LB8+XIkJCSgXbt2WLVqFQYOHIiePXti3Lhx8PT0hI2NDaZPn45Lly6VOXZTviftz+Ozzz4r9uKq2t/NqKgoPPvss9i4cSN27tyJzz77DDNnzsSGDRvK3BV7mrHYoTKrXbs2hBAIDAyU/sdiLP7+/tBoNLhw4YLO9UNSUlKQnp4Of39/o75eUbZs2QKlUonNmzfr/C+vpMNIpalduzbOnDlT6jqnTp1Chw4dyn1V2qioKHz55ZfIzMzE2rVrERAQgFatWumt16pVK7Rq1QrTpk3D6tWr0b9/f6xZswavv/56mV9L292ZPHlykf8j1XYC0tPTdU6zfrQ7ZywajQaXL1/W+Uz+888/ACCdbVS7dm3s3r0bbdq0MWoho/1cnj9/XjrsqHX+/HmzfG4L+/XXX3Hnzh1s2LABzz33nDRe+GxKS/L09ISDgwMuXryot6yoMUOEhoZi+fLluHXrFgDgxx9/RK1atbBhwwad36uYmJjHep3iXLx4EUIIndd69HP4KO0hP1dX1xKvvaTl4+OD4cOHY/jw4UhNTUWLFi0wbdo0FjtlwDk7VGYvvfQSbGxsEBsbq/e/GiEE7ty5U+5td+vWDQD0OgXabkf37t3Lve2y0v4PrvB7y8jIQFxcXLm32atXL5w6dQobN27UW6Z9naioKNy4cQNLlizRW+f+/fu4d+9eqa/Tu3dvKJVKLF++HPHx8YiKitJZnpaWpvcz0/5PsrTT24syevRouLu7Y8qUKXrLtH/ADxw4II3du3cPy5cvN/h1ymr+/PnS90IIzJ8/H3Z2dujQoQOAghyr1WpMnTpV77n5+fnlvm1DaGgoPD09sWjRIp08bt++HefOnTPL57awoj7DeXl5+Prrr80aR3FsbGwQERGBTZs26cw9uXjxIrZv317q83NycnD48OEil2mfrz2kWFQuEhISin3+47p586bO73lmZiZWrFiBZs2aFXkICwBCQkJQu3ZtfP755zpz7rT+++8/AAVd0UcPQXp6esLX17dcv79PI3Z2qMxq166NTz75BOPHj8eVK1fQs2dPVKpUCUlJSdi4cSPefPNNjB07tlzbbtq0KaKjo/HNN99IrfijR49i+fLl6NmzJ/73v/8Z+d3o69SpE+zt7REZGYmhQ4ciOzsbS5Ysgaenp/S/RUONGzcOP/74I1555RUMHjwYISEhuHv3LjZv3oxFixahadOmeO2117Bu3Tq89dZb2LdvH9q0aQO1Wo2///4b69atk673UpIWLVogKCgIH3/8MZRKpc4hLKBg7sLXX3+NF198EbVr10ZWVhaWLFkCV1dXqdA0hJubG955550iJyp36tQJNWvWxJAhQzBu3DjY2Njgu+++Q7Vq1XDt2jWDX6s0Dg4OiI+PR3R0NMLCwrB9+3Zs3boVH330kXR4ql27dhg6dCimT5+OkydPolOnTrCzs8OFCxewfv16fPnll3j55ZcNfm07OzvMnDkTgwYNQrt27dC3b1+kpKTgyy+/REBAAN59911jv90StW7dGh4eHoiOjsaoUaMgk8mwcuVKkx1GKo/Jkydj586daNOmDYYNGwa1Wo358+ejUaNGOHnyZInPzcnJQevWrdGqVSt06dIFfn5+SE9Px6ZNm/Dbb7+hZ8+eaN68OQDghRdewIYNG/Diiy+ie/fuSEpKwqJFi9CgQYMiC4vHVadOHQwZMgSJiYnw8vLCd999h5SUlBL/sySXy/Htt9+ia9euaNiwIQYNGoTq1avjxo0b2LdvH1xdXbFlyxZkZWWhRo0aePnll9G0aVO4uLhg9+7dSExMxBdffGH092KNWOyQQT788EPUqVNHmrMBFMwp6dSpE/7v//7vsbb97bffolatWli2bBk2btwIb29vjB8/3mRt50fVrVsXP/74IyZMmICxY8fC29sbw4YNQ7Vq1cp88b1Hubi44LfffkNMTAw2btyI5cuXw9PTEx06dECNGjUAFPzB27RpE+bMmYMVK1Zg48aN0v1+3nnnnTIfMuzduzemTZuGoKAgtGjRQmeZtnhcs2YNUlJS4ObmhmeeeQbff/99mSfjPmr06NGYO3eu3v847ezssHHjRgwfPhwTJ06Et7c3Ro8eDQ8PjzKfnWcIGxsbxMfHY9iwYRg3bhwqVaqEmJgYTJo0SWe9RYsWISQkBIsXL8ZHH30EW1tbBAQE4NVXXy32InVlMXDgQDg5OWHGjBn44IMP4OzsjBdffBEzZ84s9WrJxlalShX88ssveO+99zBhwgR4eHjg1VdfRYcOHcp0dpk5hISEYPv27Rg7diwmTpwIPz8/TJkyBefOnSv1bDF3d3csWbIEW7duRVxcHJKTk2FjY4O6devis88+w6hRo6R1Bw4ciOTkZCxevBg7duxAgwYNsGrVKqxfv94k9xULDg7GvHnzMG7cOJw/fx6BgYFYu3ZtqXlv3749Dh8+jKlTp2L+/PnIzs6Gt7c3wsLCMHToUACAk5MThg8fjp07d0pnxQYFBeHrr7/GsGHDjP5erJFMVKSSn4iInko9e/bEX3/9JZ2h9CQJCAhAo0aN8Msvv1g6FCoG5+wQEZFZ3b9/X+fxhQsXsG3bNrRv394yAZHV42EsIiIyq1q1amHgwIGoVasWrl69ioULF8Le3h7vv/++pUMjK8Vih4iIzKpLly744YcfkJycDIVCgfDwcHz66afFXrCU6HFxzg4RERFZNc7ZISIiIqvGYoeIiIisGufsoOBy8zdv3kSlSpXKfbl+IiIiMi8hBLKysuDr6yvdtLooLHZQcJnv0m62SERERBXT9evXpQu1FoXFDoBKlSoBKEiWq6trubejUqmwc+dO6VL0VDLmq+yYK8MwX4ZhvgzDfBnGlPnKzMyEn5+ftB8vDosdQDp05erq+tjFjpOTE1xdXfkLUAbMV9kxV4ZhvgzDfBmG+TKMOfJV2hQUTlAmIiIiq8Zih4iIiKwaix0iIiKyahYtdg4cOIDIyEj4+vpCJpNh06ZNOsuFEJg0aRJ8fHzg6OiIiIgIvTvi3r17F/3794erqyvc3d0xZMgQZGdnm/FdEBERUUVm0WLn3r17aNq0KRYsWFDk8lmzZuGrr77CokWLkJCQAGdnZ3Tu3Bm5ubnSOv3798dff/2FXbt24ZdffsGBAwfw5ptvmustEBERUQVn0bOxunbtiq5duxa5TAiBuXPnYsKECejRowcAYMWKFfDy8sKmTZvQp08fnDt3DvHx8UhMTERoaCgAYN68eejWrRs+//xz+Pr6mu29EBERUcVUYU89T0pKQnJyMiIiIqQxNzc3hIWF4fDhw+jTpw8OHz4Md3d3qdABgIiICMjlciQkJODFF18scttKpRJKpVJ6nJmZCaDg9DiVSlXumLXPfZxtPE2Yr7JjrgzDfBmG+TIM82UYU+arrNussMVOcnIyAMDLy0tn3MvLS1qWnJwMT09PneW2traoXLmytE5Rpk+fjtjYWL3xnTt3wsnJ6XFDx65dux57G08T5qvsmCvDMF+GYb4Mw3wZxhT5ysnJKdN6FbbYMaXx48djzJgx0mPtFRg7der02BcV3LVrFzp27MgLTZUB81V2zJVhmC/DMF+GYb4MY8p8aY/MlKbCFjve3t4AgJSUFPj4+EjjKSkpaNasmbROamqqzvPy8/Nx9+5d6flFUSgUUCgUeuN2dnZG+UEYaztPC+ar7JgrwzBfhmG+DMN8lU6tEfgj6S6O35ahyr9ZCA/yhI3ceDfcLmv+K+x1dgIDA+Ht7Y09e/ZIY5mZmUhISEB4eDgAIDw8HOnp6Th+/Li0zt69e6HRaBAWFmb2mImIiKhA/JlbaDtzL1797hhWXLDBq98dQ9uZexF/5pbZY7FoZyc7OxsXL16UHiclJeHkyZOoXLkyatasidGjR+OTTz5BcHAwAgMDMXHiRPj6+qJnz54AgPr166NLly544403sGjRIqhUKowcORJ9+vThmVhEREQWEn/mFoat+gPikfHkjFwMW/UHFr7aAl0a+RT5XFOwaLFz7Ngx/O9//5Mea+fRREdHY9myZXj//fdx7949vPnmm0hPT0fbtm0RHx8PBwcH6Tnff/89Ro4ciQ4dOkAul6NXr1746quvzP5eiIiInnZCCCjzNYjZ/JdeoQMAAoAMQOyWs+jYwNuoh7RKYtFip3379hCiqHQUkMlkmDJlCqZMmVLsOpUrV8bq1atNER4REZFFCCGQrxHIVwvkqTVQqTXIVwuoHnyveuT7fLUGeYXX0Qio8jU63+dr9J+nKrTtvELbKry8yBg0GqjyBfI1GuTlP3jeg+2X+t4A3MrIxdGkuwivXcX0yUQFnqBMRET0uIQQUGtEwY5bo3lQABTayWsE8vIL/lWpHywvVBzkFdr5a7/PzcvHXzdkuLT3EtSAVAw8LEb0C4r8B8WBSvNgvND3xT3P2qVm5Za+kpGw2CEiolJpi4Z8je6OXadQeKTLUJYugW6RoFsc5OVruwW63xfelurRjkYR3QrTsAGuXTLRtotmK5fBzkYOWxsZ7G3kOt/b2hQsK/jSrieHvY0MtnI57GwfjMvlsLMtGLO3lUvbLPy9XaFtPdy+7nhR62i/P3k9Ha8vP1bq+/Gs5FDqOsbCYofIDNQagaNJd5GalQvPSg54JrCy2Y5VU8WjLqU4KGuXoGA9TaGuxYPDCuqH35deaBQULHfTbfDlhUMPDp086GIU7oRoNChh1sETxVYuK7I40H4vFQfanX+h721tZLCVAcm3biDQvyYUdrZ6xYVOoVFCcWArl8HOVq5brBQuRmwefm9nI4NM9mT8zfhfXU/4uDkgOSO3yHk7MgDebgV/B82FxQ6RicWfuYXYLWdxK+Nhy9bHzQExkQ3MejaCNSpcNBSeq3A/Nw/JOcC5W1kQMnmZugSqBx2KUguNYsZLPCTySKFRMYsGGZBzz6Bn2MhlhboFJXcJtDvzoroQtg/G7QoXBw8KAZ1tyGUPtv1gzPZBcVBoG0W/TqFCQy6D/DH/o6FSqbBt23V069aA19kpgo1chpjIBhi26g/IAJ2CR5v5mMgGZv0PH4sdIhOqaKdfFkej3clrJzVqdA9BFHcIoUyHI9RFz3vQnThZ1CER/cMRjxYamhKLBlvg1GFzpfCxyGUosRCw0+kSFF0oaLsERXcjSj4EIYcGJ44dQ+vwMDgq7AstL+Fwhlz+2EUDWa8ujXyw8NUWev/R87bQf/RY7BCZiFojELvlbLGnXwLARxvPwE4uR74QeoVC4bMclKp8nLsmx9mdF6AWkDoQut2I4s7Q0J83oVNAaArmYlgD2YOiwc5GBqHOh7ODAva2NlIBoe0M6HYJSpjrUMxhjuI7FdrXkcPeVvd7/XkTD1/b0oc0VSoVci4KhAVWZqeCjKZLIx90bOCNwxdTsfO3BHR6NszoV1AuKxY7RCay51yKzv9oinL3Xh6GrCh9Il8BOXAj6fEDKwOpaHjQLSiYPyDT2UEXdwii+MmS2nkKBfMQdLsRhecmFCoOSig0ipo0qf0jWnCYYRu6dWvPnTeRBdnIZQgLrIw75wqKaUsV9ix2iIxACIF/0+4j8cpdJF5JQ+KVu7iYml2m5/p5OKJqJUXxO3a5HLZygRvXryOoVgAU9rYP5ikUPguiqEKj/GdUWLrTQERkTCx2iMpBoxE4n5KFY1fu4uiVNCQm3UVyZvmuGTHr5aalXliroFNxFd261WOngojIQCx2iMpAma/G6X8zcPTKXSQm3cXxq2nIzM3XWcdWLkOj6m5oGeCBlgGV0bymB/5v/sEKdfolEdHTiMUOUREyc1U4frWgY3PsShpO/puOvHzdi5M52dugRc2CwqZloAea+bnDyV73V6qinX5JRPQ0YrFDBCAls+A+LdrDUn8nZ+pdC6WKs/2DwqYyWgZ4oIGPK2xt5CVut6KdfklE9DRisUNPHSEELv1370FhcxeJV+7i+t37euv5V3EqKG4eHJYKrOpcriuYak+/5BWUiYgsg8UOWT2VWoOzNzMfnClVcFjqzr08nXXkMqC+j+uD4qagwPF0Nd59W2zkMrPd3ZeIiHSx2CGrk5OXjxPX0gsOS129ixPX0pGTp9ZZx95WjmZ+7njmwWGp5jXd4erAs5yIiKwRix164t3JViLxShqOPejcnLmZqXdFYFcHW535No2qu0Fha2OhiImIyJxY7NATRXvxvqNJd6XDUpf+0795oa+bA1oGVkZoQGU8E1AZwZ4uvI8PEdFTisUOVWhqjcD55CypsEm8chcpmUq99ep4uUiFTWiAB2p4OFkgWiIiqohY7FCFkqtS489/M6TC5vjVNGQVcfG+xjXcHhQ2lRHq7wEPZ3sLRUxERBUdix2yqJx84Nd//sMf1zORmHQXf/6bgTy17sX7nO1t0MLfQypumvm5w9Ge822IiKhsWOyQWSVn5Eq3XEhMuoPzKTYQiSd01qnqopCubfNMYGXU865U6sX7iIiIisNih0ym4OJ92TialCZdwO/ftEcv3idDgPbifYEF17gJqOJUrov3ERERFYXFDhmNSq3BmRsZOHYlDUevFNx6IS1HpbOOXAY08C24eF/zGq7IuPgH+vRsyzt5ExGRybDYeYqpNeKxbmFwT/ng4n0PCpsT19JxX6V78T6FrRzNa7pLVyZu4e8BF0XBx06lUmHbNaO+JSIiIj0sdp5S8Wdu6d2c0qeUm1PezlY+uHBfGhKv3MVfRVy8z83RTppv0zKwMhr5usHelvNtiIjIcljsPIXiz9zCsFV/4JGbeiM5IxfDVv2Bha+2QOeG3rh2N6egsHlwAb/Lt/Uv3lfd3bGguHkw3yaoGi/eR0REFQuLnaeMWiMQu+WsXqEDQBobvfYkKils8V92nt46db0qoWVgQecmNKAyqrs7mjReIiKix8Vi5ylzNOmuzqGrouSqNMhV5cHORoYmNdwRGlBwjZsQfw+4O/HifURE9GRhsfOUSc0qudDRGvV8EIb/LwgOdrx4HxERPdk4c/Qp41nJoUzrhdeuykKHiIisAoudp8wzgZVRyaH4hp4MBWdlPRNY2XxBERERmRCLnafMNwcu691YU0t7DlVMZAODrrdDRERUkbHYeYp8tecCZsb/DQDo3tgH3m66h7S83Ryw8NUWxV5nh4iI6EnECcpPASEEvtj5D+bvuwgAGNe5Lkb8L+ixr6BMRET0JGCxY+WEEJi+/W98c+AyAODjbvXxxnO1AAA2chnCa1exZHhEREQmx2LHiglRcAHBZb9fAQBMjmyAgW0CLRsUERGRmbHYsVIajcCEn89gdULBnTY/fbEx+oXVtHBURERE5sdixwqpNQIf/vQn1h//FzIZMLNXE0SF+lk6LCIiIotgsWNl8tUajF1/CptO3oSNXIbZUU3Ro1l1S4dFRERkMSx2rIhKrcHoNSex9fQt2Mpl+Kpvc3RrzNPIiYjo6cZix0oo89V4e/UJ7DybAjsbGRb0a4FODb0tHRYREZHFsdixArkqNYatOo595/+Dva0ci18Lwf/qelo6LCIiogqBxc4T7n6eGm+sOIaDF2/DwU6Obwe0RNvgqpYOi4iIqMJgsfMEu6fMx5DliThy+S6c7G3w3cCWaFWLFwkkIiIqjMXOEyorV4VBcYk4djUNLgpbLBvUEqEBvFM5ERHRo1jsPIEyclQYEHcUp66nw9XBFiuGhKGZn7ulwyIiIqqQWOw8YdLu5eHVpQn462Ym3J3ssGpIGBpVd7N0WERERBUWi50nyO1sJV79NgF/J2ehirM9vn8jDPW8XS0dFhERUYXGYucJkZqZi37fJuBiajaqVVJg9ethCPaqZOmwiIiIKjy5pQMoTVZWFkaPHg1/f384OjqidevWSExMlJYLITBp0iT4+PjA0dERERERuHDhggUjNr5bGffR+5sjuJiaDW9XB6x9sxULHSIiojKq8MXO66+/jl27dmHlypU4ffo0OnXqhIiICNy4cQMAMGvWLHz11VdYtGgREhIS4OzsjM6dOyM3N9fCkRvHv2k56L34CJJu30N1d0esGxqOWtVcLB0WERHRE6NCFzv379/HTz/9hFmzZuG5555DUFAQJk+ejKCgICxcuBBCCMydOxcTJkxAjx490KRJE6xYsQI3b97Epk2bLB3+Y7t65x56Lz6Ca3dz4F/FCWuHtkLNKk6WDouIiOiJUqHn7OTn50OtVsPBwUFn3NHREQcPHkRSUhKSk5MREREhLXNzc0NYWBgOHz6MPn36FLldpVIJpVIpPc7MzAQAqFQqqFSqcserfe7jbEPr8n/3MGDZMaRkKlGrqhOWDwqFl4udUbZdURgzX9aOuTIM82UY5sswzJdhTJmvsm5TJoQQRn91I2rdujXs7e2xevVqeHl54YcffkB0dDSCgoIQFxeHNm3a4ObNm/DxeXh376ioKMhkMqxdu7bIbU6ePBmxsbF646tXr4aTk+U7J8k5wPyzNshSyeDtKDCigRqu9paOioiIqGLJyclBv379kJGRAVfX4s9OrtCdHQBYuXIlBg8ejOrVq8PGxgYtWrRA3759cfz48XJvc/z48RgzZoz0ODMzE35+fujUqVOJySqNSqXCrl270LFjR9jZ2ZW6vlojcOxqGlKzlPCspECovwcupGYjdtkxZKlUqOddCcsGhqCKs3VWOobm62nGXBmG+TIM82UY5sswpsyX9shMaSp8sVO7dm3s378f9+7dQ2ZmJnx8fNC7d2/UqlUL3t7eAICUlBSdzk5KSgqaNWtW7DYVCgUUCoXeuJ2dnVF+EGXZTvyZW4jdcha3Mh5OpK7qYo+cPDVy8tRoXN0NK4c8A3cn6yx0CjNW3p8GzJVhmC/DMF+GYb4MY4p8lXV7FXqCcmHOzs7w8fFBWloaduzYgR49eiAwMBDe3t7Ys2ePtF5mZiYSEhIQHh5uwWhLFn/mFoat+kOn0AGA29l5yMlTI7CqE1a9HvZUFDpERESmVuE7Ozt27IAQAnXr1sXFixcxbtw41KtXD4MGDYJMJsPo0aPxySefIDg4GIGBgZg4cSJ8fX3Rs2dPS4deJLVGIHbLWZQ0Uep+ngYuigr/oyEiInoiVPg9akZGBsaPH49///0XlStXRq9evTBt2jSpdfX+++/j3r17ePPNN5Geno62bdsiPj5e7wyuiuJo0l29js6jkjNzcTTpLsJrVzFTVERERNarwhc7UVFRiIqKKna5TCbDlClTMGXKFDNGVX6pWWW72GFZ1yMiIqKSPTFzdqyFZ6WydZzKuh4RERGVjMWOmT0TWBk+bg6QFbNcBsDHzQHPBFY2Z1hERERWi8WOmdnIZYiJbAAAegWP9nFMZAPYyIsrh4iIiMgQLHYsoEsjHyx8tQW83XQPVXm7OWDhqy3QpZFPMc8kIiIiQ1X4CcrWqksjH3Rs4I3mU3YhM1eFGS81xiuhfuzoEBERGRk7OxZkI5dBPLjiTsvAyix0iIiITIDFjoUp8zUAAAc7GwtHQkREZJ1Y7FiQRiOQ96DYUdjyR0FERGQK3MNaUJ5aI33Pzg4REZFpsNixoFyVWvqenR0iIiLT4B7WgrTzdWzkMtjZ8EdBRERkCtzDWpC2s8OuDhERkelwL2tBPBOLiIjI9FjsWJBSxTOxiIiITI17WQvKzedhLCIiIlPjXtaCtJ0dHsYiIiIyHRY7FsQJykRERKbHvawFaScoK9jZISIiMhkWOxbEzg4REZHpcS9rQTz1nIiIyPRY7FgQOztERESmx72sBUlzdmzZ2SEiIjIVFjsWpHxwnR0HO/4YiIiITIV7WQvKVbGzQ0REZGosdiyInR0iIiLT417WgtjZISIiMj0WOxbEzg4REZHpcS9rQbzrORERkelxL2tBDzs7PIxFRERkKix2LOjhvbH4YyAiIjIV7mUt6OEVlNnZISIiMhUWOxb08N5Y/DEQERGZCveyFsTODhERkemx2LEgdnaIiIhMj3tZC2Jnh4iIyPRY7FgQOztERESmx72sBbGzQ0REZHosdixECPHwOju8gjIREZHJcC9rISq1gBAF3yt4BWUiIiKTYbFjIbkPbhUBsLNDRERkStzLWoj2JqAAix0iIiJT4l7WQh5OTpZDJpNZOBoiIiLrxWLHQh6eds75OkRERKbEYsdCCnd2iIiIyHS4p7UQdnaIiIjMg8WOhSjz2dkhIiIyB+5pLUR7NpaCt4ogIiIyKe5pLUTb2XHgrSKIiIhMyrY8T7p27RquXr2KnJwcVKtWDQ0bNoRCoTB2bFYtl50dIiIisyjznvbKlSv44IMP4O/vj8DAQLRr1w5du3ZFaGgo3Nzc0LFjR6xfvx4ajab0jZWRWq3GxIkTERgYCEdHR9SuXRtTp06F0N5nAQX3mJo0aRJ8fHzg6OiIiIgIXLhwwWgxmAo7O0REROZRpmJn1KhRaNq0KZKSkvDJJ5/g7NmzyMjIQF5eHpKTk7Ft2za0bdsWkyZNQpMmTZCYmGiU4GbOnImFCxdi/vz5OHfuHGbOnIlZs2Zh3rx50jqzZs3CV199hUWLFiEhIQHOzs7o3LkzcnNzjRKDqbCzQ0REZB5lOozl7OyMy5cvo0qVKnrLPD098fzzz+P5559HTEwM4uPjcf36dbRs2fKxg/v999/Ro0cPdO/eHQAQEBCAH374AUePHgVQ0NWZO3cuJkyYgB49egAAVqxYAS8vL2zatAl9+vR57BhMhZ0dIiIi8yhTsTN9+vQyb7BLly7lDuZRrVu3xjfffIN//vkHderUwalTp3Dw4EHMnj0bAJCUlITk5GRERERIz3Fzc0NYWBgOHz5cbLGjVCqhVCqlx5mZmQAAlUoFlUpV7ni1zy3LNnKU+QAAO5uyrW+NDMnX0465MgzzZRjmyzDMl2FMma+ybrNcE5S1bt++jYSEBKjVarRs2RI+Pj6Pszk9H374ITIzM1GvXj3Y2NhArVZj2rRp6N+/PwAgOTkZAODl5aXzPC8vL2lZUaZPn47Y2Fi98Z07d8LJyemx4961a1ep6/x1TQ5Ajlv/Xse2bVcf+zWfZGXJFxVgrgzDfBmG+TIM82UYU+QrJyenTOuVu9j56aefMGTIENSpUwcqlQrnz5/HggULMGjQoPJuUs+6devw/fffY/Xq1WjYsCFOnjyJ0aNHw9fXF9HR0eXe7vjx4zFmzBjpcWZmJvz8/NCpUye4urqWe7sqlQq7du1Cx44dYWdnV+K6p7afB25cRd2gWujWuU65X/NJZki+nnbMlWGYL8MwX4Zhvgxjynxpj8yUpszFTnZ2NlxcXKTHsbGxOHr0KOrUKdhRb926FW+88YZRi51x48bhww8/lA5HNW7cGFevXsX06dMRHR0Nb29vAEBKSopOVyklJQXNmjUrdrsKhaLIU+Xt7OyM8oMoy3byNAVnlDkqjPOaTzJj5f1pwFwZhvkyDPNlGObLMKbIV1m3V+ZTgUJCQvDzzz9Lj21tbZGamio9TklJgb29vQEhli4nJwdyuW6INjY20untgYGB8Pb2xp49e6TlmZmZSEhIQHh4uFFjMTbtFZQdeDYWERGRSZW5s7Njxw6MGDECy5Ytw4IFC/Dll1+id+/eUKvVyM/Ph1wux7Jly4waXGRkJKZNm4aaNWuiYcOGOHHiBGbPno3BgwcDAGQyGUaPHo1PPvkEwcHBCAwMxMSJE+Hr64uePXsaNRZjy31wI1AFz8YiIiIyqTIXOwEBAdi6dSt++OEHtGvXDqNGjcLFixdx8eJFqNVq1KtXDw4ODkYNbt68eZg4cSKGDx+O1NRU+Pr6YujQoZg0aZK0zvvvv4979+7hzTffRHp6Otq2bYv4+Hijx2JsStWDU8/Z2SEiIjIpg/e0ffv2RWJiIk6dOoX27dtDo9GgWbNmJikuKlWqhLlz5+Lq1au4f/8+Ll26hE8++UTncJlMJsOUKVOQnJyM3Nxc7N69W5pHVJGxs0NERGQeBp2NtW3bNpw7dw5NmzbFt99+i/3796N///7o2rUrpkyZAkdHR1PFaXXY2SEiIjKPMu9p33vvPQwaNAiJiYkYOnQopk6dinbt2uGPP/6Ag4MDmjdvju3bt5syVquiZGeHiIjILMpc7Cxbtgzbtm3DmjVrkJiYiJUrVwIA7O3tMXXqVGzYsAGffvqpyQK1NrkPOjsKW3Z2iIiITKnMe1pnZ2ckJSUBAK5fv643R6dBgwb47bffjBudFcvL1556zs4OERGRKZW52Jk+fToGDBgAX19ftGvXDlOnTjVlXFaPnR0iIiLzKPME5f79+6NLly64fPkygoOD4e7ubsKwrJ+SnR0iIiKzMOhsrCpVqqBKlSqmiuWpws4OERGReZRpT/vWW2/h33//LdMG165di++///6xgnoasLNDRERkHmXq7FSrVg0NGzZEmzZtEBkZidDQUPj6+sLBwQFpaWk4e/YsDh48iDVr1sDX1xfffPONqeN+ouWrNch/cCNQdnaIiIhMq0zFztSpUzFy5Eh8++23+Prrr3H27Fmd5ZUqVUJERAS++eYbdOnSxSSBWhNtVwcAFLyoIBERkUmVec6Ol5cXPv74Y3z88cdIS0vDtWvXcP/+fVStWhW1a9eGTCYzZZxWRafY4UUFiYiITMqgCcpaHh4e8PDwMHYsTw3t5GQ7Gxls5CwSiYiITInHUCxAmpzMrg4REZHJsdixAOm0c87XISIiMjnubS2ANwElIiIyHxY7FqBkZ4eIiMhsDN7bxsTE4OrVq6aI5amRyzk7REREZmNwsfPzzz+jdu3a6NChA1avXg2lUmmKuKwaOztERETmY/De9uTJk0hMTETDhg3xzjvvwNvbG8OGDUNiYqIp4rNKudKcHRY7REREplauvW3z5s3x1Vdf4ebNm1i6dCn+/fdftGnTBk2aNMGXX36JjIwMY8dpVbSdHd4Xi4iIyPQeq7UghIBKpUJeXh6EEPDw8MD8+fPh5+eHtWvXGitGq8PODhERkfmUa297/PhxjBw5Ej4+Pnj33XfRvHlznDt3Dvv378eFCxcwbdo0jBo1ytixWg12doiIiMzH4GKncePGaNWqFZKSkrB06VJcv34dM2bMQFBQkLRO37598d9//xk1UGuiZGeHiIjIbAy+N1ZUVBQGDx6M6tWrF7tO1apVodFoil3+tGNnh4iIyHwMLnYmTpxoijieKuzsEBERmY/Be9tevXph5syZeuOzZs3CK6+8YpSgrJ10byxeVJCIiMjkDC52Dhw4gG7duumNd+3aFQcOHDBKUNZOuus5LypIRERkcgbvbbOzs2Fvb683bmdnh8zMTKMEZe3Y2SEiIjKfcp2NVdQ1dNasWYMGDRoYJShrx84OERGR+ZRrgvJLL72ES5cu4fnnnwcA7NmzBz/88APWr19v9ACtETs7RERE5mNwsRMZGYlNmzbh008/xY8//ghHR0c0adIEu3fvRrt27UwRo9WRzsZiZ4eIiMjkDC52AKB79+7o3r27sWN5ajw89ZydHSIiIlNja8ECcqWLCjL9REREpmZwZ0etVmPOnDlYt24drl27hry8PJ3ld+/eNVpw1oqdHSIiIvMxuLUQGxuL2bNno3fv3sjIyMCYMWPw0ksvQS6XY/LkySYI0fpIE5TZ2SEiIjI5g/e233//PZYsWYL33nsPtra26Nu3L7799ltMmjQJR44cMUWMVkc69ZydHSIiIpMzuNhJTk5G48aNAQAuLi7IyMgAALzwwgvYunWrcaOzUuzsEBERmY/Be9saNWrg1q1bAIDatWtj586dAIDExEQoFArjRmelHl5UkJ0dIiIiUzO42HnxxRexZ88eAMDbb7+NiRMnIjg4GAMGDMDgwYONHqC10WgE8njXcyIiIrMx+GysGTNmSN/37t0b/v7++P333xEcHIzIyEijBmeN8tQa6Xt2doiIiEzPoGJHpVJh6NChmDhxIgIDAwEArVq1QqtWrUwSnDVSqh4WO+zsEBERmZ5Be1s7Ozv89NNPporlqZCbXzA5WS4DbOUyC0dDRERk/QxuLfTs2RObNm0yQShPB21nx8HOBjIZix0iIiJTM3jOTnBwMKZMmYJDhw4hJCQEzs7OOstHjRpltOCskbazw0NYRERE5mFwsbN06VK4u7vj+PHjOH78uM4ymUzGYqcUhTs7REREZHoGFztJSUmmiOOpwc4OERGReXGPa2bs7BAREZmXwZ2d0i4c+N1335U7mKeBkp0dIiIiszJ4j5uWlqbzlZqair1792LDhg1IT083eoABAQGQyWR6XyNGjAAA5ObmYsSIEahSpQpcXFzQq1cvpKSkGD0OY8l90NlRsLNDRERkFgZ3djZu3Kg3ptFoMGzYMNSuXdsoQRWWmJgItVotPT5z5gw6duyIV155BQDw7rvvYuvWrVi/fj3c3NwwcuRIvPTSSzh06JDRYzEGdnaIiIjMyyh7XLlcjjFjxmDOnDnG2JyOatWqwdvbW/r65ZdfULt2bbRr1w4ZGRlYunQpZs+ejeeffx4hISGIi4vD77//jiNHjhg9FmOQOju27OwQERGZg9HaC5cuXUJ+fr6xNlekvLw8rFq1CoMHD4ZMJsPx48ehUqkQEREhrVOvXj3UrFkThw8fNmks5aXt7DjYsbNDRERkDgYfxhozZozOYyEEbt26ha1btyI6OtpogRVl06ZNSE9Px8CBAwEAycnJsLe3h7u7u856Xl5eSE5OLnY7SqUSSqVSepyZmQmg4N5fKpWq3PFpn1vSNu7lFiyzs5E91mtZg7LkiwowV4ZhvgzDfBmG+TKMKfNV1m0aXOycOHFC57FcLke1atXwxRdflHqm1uNaunQpunbtCl9f38fazvTp0xEbG6s3vnPnTjg5OT3WtgFg165dxS47c10GwAapN//Ftm3XHvu1rEFJ+SJdzJVhmC/DMF+GYb4MY4p85eTklGk9g4udffv2GRyMMVy9ehW7d+/Ghg0bpDFvb2/k5eUhPT1dp7uTkpICb2/vYrc1fvx4nQ5VZmYm/Pz80KlTJ7i6upY7RpVKhV27dqFjx46ws7Mrcp2zOy8A/yYhuFYAunWrV+7XsgZlyRcVYK4Mw3wZhvkyDPNlGFPmS3tkpjTluoJyfn4+goODdcYvXLgAOzs7BAQEGLrJMomLi4Onpye6d+8ujYWEhMDOzg579uxBr169AADnz5/HtWvXEB4eXuy2FAoFFAqF3ridnZ1RfhAlbSdPIwAATgrjvJY1MFbenwbMlWGYL8MwX4ZhvgxjinyVdXsGz5IdOHAgfv/9d73xhIQEaS6NsWk0GsTFxSE6Ohq2tg/rMzc3NwwZMgRjxozBvn37cPz4cQwaNAjh4eFo1aqVSWJ5XMp8no1FRERkTgYXOydOnECbNm30xlu1aoWTJ08aIyY9u3fvxrVr14qcEzRnzhy88MIL6NWrF5577jl4e3vrHOqqaHJVD66zw7OxiIiIzMLgw1gymQxZWVl64xkZGToX/zOmTp06QQhR5DIHBwcsWLAACxYsMMlrG5u2s+PAiwoSERGZhcF73Oeeew7Tp0/XKWzUajWmT5+Otm3bGjU4a6SUOjs8jEVERGQOBnd2Zs6cieeeew5169bFs88+CwD47bffkJmZib179xo9QGsjdXZ4GIuIiMgsDN7jNmjQAH/++SeioqKQmpqKrKwsDBgwAH///TcaNWpkihitijRnhxOUiYiIzMLgzg4A+Pr64tNPPzV2LE8FdnaIiIjMy+A9blxcHNavX683vn79eixfvtwoQVkzJW8ESkREZFYGFzvTp09H1apV9cY9PT3Z7SmDXN4IlIiIyKwM3uNeu3YNgYGBeuP+/v64do33eioNOztERETmZXCx4+npiT///FNv/NSpU6hSpYpRgrJm2s6OgtfZISIiMguD97h9+/bFqFGjsG/fPqjVaqjVauzduxfvvPMO+vTpY4oYrYq2s+PA6+wQERGZhcFnY02dOhVXrlxBhw4dpPtUaTQaDBgwANOmTTN6gNZECMHODhERkZkZXOzY29tj7dq1+OSTT3Dy5Ek4OjqicePG8Pf3N0V8VkWlFtDe9YJXUCYiIjKPcl1nBwCCg4MRHBwMAMjMzMTChQuxdOlSHDt2zGjBWRtl/sNbbLCzQ0REZB7lLnYAYN++ffjuu++wYcMGuLm54cUXXzRWXFYp98F8HYDFDhERkbkYXOzcuHEDy5YtQ1xcHNLT05GWlobVq1cjKioKMpnMFDFaDWWh+TrMFRERkXmUub3w008/oVu3bqhbty5OnjyJL774Ajdv3oRcLkfjxo258y6DXOkaO+zqEBERmUuZOzu9e/fGBx98gLVr16JSpUqmjMlqKaWrJ3NyMhERkbmUucUwZMgQLFiwAF26dMGiRYuQlpZmyrisktTZ4a0iiIiIzKbMe93Fixfj1q1bePPNN/HDDz/Ax8cHPXr0gBACGo2m9A3Qw84ObxVBRERkNga1GBwdHREdHY39+/fj9OnTaNiwIby8vNCmTRv069cPGzZsMFWcVkHJzg4REZHZlXuvGxwcjE8//RTXr1/HqlWrkJOTg759+xozNqvDzg4REZH5PdZ1dgBALpcjMjISkZGRSE1NNUZMVkuZz84OERGRuRl1r+vp6WnMzVmdXBU7O0RERObGFoMZsbNDRERkftzrmpG2s6NgZ4eIiMhsWOyYkfZsLAd2doiIiMzG4L1urVq1cOfOHb3x9PR01KpVyyhBWavcfHZ2iIiIzM3gYufKlStQq9V640qlEjdu3DBKUNaK19khIiIyvzKfer5582bp+x07dsDNzU16rFarsWfPHgQEBBg1OGsjTVBmZ4eIiMhsylzs9OzZEwAgk8kQHR2ts8zOzg4BAQH44osvjBqctZFOPWdnh4iIyGzKXOxo738VGBiIxMREVK1a1WRBWSt2doiIiMzP4CsoJyUl6Y2lp6fD3d3dGPFYtYennrOzQ0REZC4G73VnzpyJtWvXSo9feeUVVK5cGdWrV8epU6eMGpy10XZ2HOzY2SEiIjIXg4udRYsWwc/PDwCwa9cu7N69G/Hx8ejatSvGjRtn9ACtCTs7RERE5mfwYazk5GSp2Pnll18QFRWFTp06ISAgAGFhYUYP0Jqws0NERGR+BrcYPDw8cP36dQBAfHw8IiIiAABCiCKvv0MPPZygzM4OERGRuRjc2XnppZfQr18/BAcH486dO+jatSsA4MSJEwgKCjJ6gNZEKZ16zs4OERGRuRhc7MyZMwcBAQG4fv06Zs2aBRcXFwDArVu3MHz4cKMHaE3Y2SEiIjI/g4sdOzs7jB07Vm/83XffNUpA1iyXnR0iIiKzK1eLYeXKlWjbti18fX1x9epVAMDcuXPx888/GzU4a8PODhERkfkZvNdduHAhxowZg65duyI9PV2alOzu7o65c+caOz6rIp16zttFEBERmY3Be9158+ZhyZIl+Pjjj2Fj8/BwTGhoKE6fPm3U4KxJvlqDfI0AADjwdhFERERmY3Cxk5SUhObNm+uNKxQK3Lt3zyhBWSPtISyAnR0iIiJzMnivGxgYiJMnT+qNx8fHo379+saIySrpFDvs7BAREZlNmc/GmjJlCsaOHYsxY8ZgxIgRyM3NhRACR48exQ8//IDp06fj22+/NWWsTzRlfsF8HTsbGWzkMgtHQ0RE9PQoc7ETGxuLt956C6+//jocHR0xYcIE5OTkoF+/fvD19cWXX36JPn36mDLWJ1qu6sGtItjVISIiMqsyFztCCOn7/v37o3///sjJyUF2djY8PT1NEpw10XZ2OF+HiIjIvAy6qKBMpnv4xcnJCU5OTkYNyFppOzucr0NERGReBhU7derU0St4HnX37t3HCshaKXmNHSIiIoswqNiJjY2Fm5ubqWKxarn57OwQERFZgkHFTp8+fcw+P+fGjRv44IMPsH37duTk5CAoKAhxcXEIDQ0FUDCXKCYmBkuWLEF6ejratGmDhQsXIjg42KxxlubhHc/Z2SEiIjKnMu95Szt8ZQppaWlo06YN7OzssH37dpw9exZffPEFPDw8pHVmzZqFr776CosWLUJCQgKcnZ3RuXNn5Obmmj3ekvC+WERERJZRrrOxzGXmzJnw8/NDXFycNBYYGKgT09y5czFhwgT06NEDALBixQp4eXlh06ZNFepUeN7xnIiIyDLK3GbQaDRmP4S1efNmhIaG4pVXXoGnpyeaN2+OJUuWSMuTkpKQnJyMiIgIaczNzQ1hYWE4fPiwWWMtDTs7RERElmHQnB1zu3z5snSX9Y8++giJiYkYNWoU7O3tER0djeTkZACAl5eXzvO8vLykZUVRKpVQKpXS48zMTACASqWCSqUqd7za5xa1jRxlwZi9jeyxXsOalJQv0sVcGYb5MgzzZRjmyzCmzFdZtykTljg+VUb29vYIDQ3F77//Lo2NGjUKiYmJOHz4MH7//Xe0adMGN2/ehI+Pj7ROVFQUZDIZ1q5dW+R2J0+ejNjYWL3x1atXm+y6QbtuyPDLNRuEVdOgX5Cm9CcQERFRibR3csjIyICrq2ux61Xozo6Pjw8aNGigM1a/fn389NNPAABvb28AQEpKik6xk5KSgmbNmhW73fHjx2PMmDHS48zMTPj5+aFTp04lJqs0KpUKu3btQseOHWFnZ6ez7J89F4FrlxEU6I9u3XjDVKDkfJEu5sowzJdhmC/DMF+GMWW+tEdmSlOhi502bdrg/PnzOmP//PMP/P39ARRMVvb29saePXuk4iYzMxMJCQkYNmxYsdtVKBRQKBR643Z2dkb5QRS1He1Nz50UtvzleISx8v40YK4Mw3wZhvkyDPNlGFPkq6zbq9DFzrvvvovWrVvj008/RVRUFI4ePYpvvvkG33zzDYCC0+FHjx6NTz75BMHBwQgMDMTEiRPh6+uLnj17Wjb4R2jPxuJFBYmIiMyrQhc7LVu2xMaNGzF+/HhMmTIFgYGBmDt3Lvr37y+t8/777+PevXt48803kZ6ejrZt2yI+Ph4ODg4WjFyf9mwsXlSQiIjIvCp0sQMAL7zwAl544YVil8tkMkyZMgVTpkwxY1SGU/J2EURERBbBNoOZ5PJ2EURERBbBPa+ZsLNDRERkGSx2zESaoMzODhERkVlxz2sm7OwQERFZBosdM2Fnh4iIyDK45zUT6dRzdnaIiIjMisWOmSjz2dkhIiKyBO55zSRXxc4OERGRJbDYMRMl5+wQERFZBPe8ZpIr3S6CnR0iIiJzYrFjBkII5EmnnjPlRERE5sQ9rxloz8QCWOwQERGZG/e8ZqBUPSx2eBiLiIjIvFjsmIH2tHO5DLCVyywcDRER0dOFxY4ZSKed29lAJmOxQ0REZE4sdsxAuqAg5+sQERGZHfe+ZlC4s0NERETmxWLHDNjZISIishzufc2AnR0iIiLLYbFjBuzsEBERWQ73vmag7ewoeBNQIiIis2OxYwZSZ4c3ASUiIjI77n3NQJnPzg4REZGlsNgxg1xVQWfHgZ0dIiIis+Pe1wzY2SEiIrIcFjtmwM4OERGR5XDvawbs7BAREVkOix0z0HZ2eDYWERGR+XHvawbazo4DOztERERmx2LHDJTaiwqys0NERGR23PuaQe6Diwo68HYRREREZse9rxk87OzwMBYREZG5sdgxA+3tInjqORERkflx72sGSt4IlIiIyGJY7JhBLjs7REREFsO9rxmws0NERGQ5LHbMQNvZUfBsLCIiIrPj3tcMtJ0dB56NRUREZHYsdsxAyc4OERGRxXDvawa57OwQERFZDIsdExNCsLNDRERkQdz7mphKLaARBd/zCspERETmx2LHxLRdHYCdHSIiIkvg3tfEtPN1ABY7RERElsC9r4kVnq8jk8ksHA0REdHTh8WOiSnztVdPZqqJiIgsgXtgE8tVae+LxcnJRERElsBix8Skzg5vAkpERGQR3AObmNTZ4U1AiYiILKJCFzuTJ0+GTCbT+apXr560PDc3FyNGjECVKlXg4uKCXr16ISUlxYIR62Nnh4iIyLIq/B64YcOGuHXrlvR18OBBadm7776LLVu2YP369di/fz9u3ryJl156yYLR6lOys0NERGRRtpYOoDS2trbw9vbWG8/IyMDSpUuxevVqPP/88wCAuLg41K9fH0eOHEGrVq3MHWqR2NkhIiKyrAq/B75w4QJ8fX1Rq1Yt9O/fH9euXQMAHD9+HCqVChEREdK69erVQ82aNXH48GFLhatHqdKees7ODhERkSVU6M5OWFgYli1bhrp16+LWrVuIjY3Fs88+izNnziA5ORn29vZwd3fXeY6XlxeSk5NL3K5SqYRSqZQeZ2ZmAgBUKhVUKlW549U+t/A27inzAAD2NrLH2rY1KipfVDTmyjDMl2GYL8MwX4YxZb7Kuk2ZEEIY/dVNJD09Hf7+/pg9ezYcHR0xaNAgnaIFAJ555hn873//w8yZM4vdzuTJkxEbG6s3vnr1ajg5ORk15r03Zfj5qg1Cq2rwWrCm9CcQERFRmeTk5KBfv37IyMiAq6trsetV6M7Oo9zd3VGnTh1cvHgRHTt2RF5eHtLT03W6OykpKUXO8Sls/PjxGDNmjPQ4MzMTfn5+6NSpU4nJKo1KpcKuXbvQsWNH2NnZAQCSfr0MXL2I2gF+6NatYbm3bY2KyhcVjbkyDPNlGObLMMyXYUyZL+2RmdI8UcVOdnY2Ll26hNdeew0hISGws7PDnj170KtXLwDA+fPnce3aNYSHh5e4HYVCAYVCoTduZ2dnlB9E4e08mJ8MR3vjbNsaGSvvTwPmyjDMl2GYL8MwX4YxRb7Kur0KXeyMHTsWkZGR8Pf3x82bNxETEwMbGxv07dsXbm5uGDJkCMaMGYPKlSvD1dUVb7/9NsLDwyvMmVjAw4sK8mwsIiIiy6jQxc6///6Lvn374s6dO6hWrRratm2LI0eOoFq1agCAOXPmQC6Xo1evXlAqlejcuTO+/vprC0et6+GNQHk2FhERkSVU6GJnzZo1JS53cHDAggULsGDBAjNFZDips8O7nhMREVkE98Ampu3s8K7nRERElsFix8SU+ezsEBERWRL3wCaWq2Jnh4iIyJJY7JgYOztERESWxT2wibGzQ0REZFksdkzs4annTDUREZElcA9sYsoHp56zs0NERGQZLHZMjJ0dIiIiy+Ie2MSkCcq8XQQREZFFcA9sYtIEZd4ugoiIyCJY7JgYOztERESWxT2wCak1Aiq1AMDODhERkaWw2DEhbVcHYGeHiIjIUrgHNiHtfB0AULCzQ0REZBEsdkxI29mxs5HBRi6zcDRERERPJxY7JqTt7LCrQ0REZDksdkxI29lx4HwdIiIii+Fe2ISU7OwQERFZHIsdE8pV8Ro7RERElsa9sAk9vC8WOztERESWwmLHhHJVnLNDRERkadwLmxDveE5ERGR53Aub0MPODg9jERERWQqLHRNiZ4eIiMjyuBc2IU5QJiIisjwWOybECcpERESWx72wCbGzQ0REZHksdkxIyc4OERGRxXEvbELs7BAREVkeix0T4pwdIiIiy+Ne2ITY2SEiIrI8FjsmpMznjUCJiIgsjXthE1FrBG6l5wIAbqTdh1ojLBwRERHR04nFjgns+CsFbWfuxYnr6QCAxQcuo+3MvYg/c8uygRERET2FWOwY2ak7Mry95hRuZeTqjCdn5GLYqj9Y8BAREZkZix0jUmsENlyRo6gDVtqx2C1neUiLiIjIjFjsGNGxq2lIz5MVu1wAuJWRi6NJd80XFBER0VOOxY4RpWYpy7hebukrERERkVGw2DEiz0qKMq7nYOJIiIiISIvFjhGF+nvA3V6guANZMgA+bg54JrCyOcMiIiJ6qrHYMSIbuQwvBRRcNfnRgkf7OCayAWzkxc/rISIiIuNisWNkTasIzOvTFN5uuoeqvN0csPDVFujSyMdCkRERET2dbC0dgDXq3NALXZtUx9Gku0jNyoVnpYJDV+zoEBERmR+LHROxkcsQXruKpcMgIiJ66vEwFhEREVk1FjtERERk1VjsEBERkVVjsUNERERWjcUOERERWbUnqtiZMWMGZDIZRo8eLY3l5uZixIgRqFKlClxcXNCrVy+kpKRYLkgiIiKqUJ6YYicxMRGLFy9GkyZNdMbfffddbNmyBevXr8f+/ftx8+ZNvPTSSxaKkoiIiCqaJ6LYyc7ORv/+/bFkyRJ4eHhI4xkZGVi6dClmz56N559/HiEhIYiLi8Pvv/+OI0eOWDBiIiIiqiieiIsKjhgxAt27d0dERAQ++eQTafz48eNQqVSIiIiQxurVq4eaNWvi8OHDaNWqVZHbUyqVUCqV0uPMzEwAgEqlgkqlKnec2uc+zjaeJsxX2TFXhmG+DMN8GYb5Mowp81XWbVb4YmfNmjX4448/kJiYqLcsOTkZ9vb2cHd31xn38vJCcnJysducPn06YmNj9cY3bdoEJyenx475559/fuxtPE2Yr7JjrgzDfBmG+TIM82UYU+QrJycHACCEKHG9Cl3sXL9+He+88w527doFBweH0p9QRuPHj8eYMWOkxzdu3ECDBg3w+uuvG+01iIiIyDyysrLg5uZW7PIKXewcP34cqampaNGihTSmVqtx4MABzJ8/Hzt27EBeXh7S09N1ujspKSnw9vYudrsKhQIKhUJ67OLiguvXr6NSpUqQycp/s87MzEz4+fnh+vXrcHV1Lfd2nhbMV9kxV4ZhvgzDfBmG+TKMKfMlhEBWVhZ8fX1LXK9CFzsdOnTA6dOndcYGDRqEevXq4YMPPoCfnx/s7OywZ88e9OrVCwBw/vx5XLt2DeHh4WV+Hblcjho1ahgtbldXV/4CGID5KjvmyjDMl2GYL8MwX4YxVb5K6uhoVehip1KlSmjUqJHOmLOzM6pUqSKNDxkyBGPGjEHlypXh6uqKt99+G+Hh4cVOTiYiIqKnS4Uudspizpw5kMvl6NWrF5RKJTp37oyvv/7a0mERERFRBfHEFTu//vqrzmMHBwcsWLAACxYssExAhSgUCsTExOjMB6LiMV9lx1wZhvkyDPNlGObLMBUhXzJR2vlaRERERE+wJ+IKykRERETlxWKHiIiIrBqLHSIiIrJqLHaIiIjIqrHYMZIFCxYgICAADg4OCAsLw9GjRy0dktEdOHAAkZGR8PX1hUwmw6ZNm3SWCyEwadIk+Pj4wNHREREREbhw4YLOOnfv3kX//v3h6uoKd3d3DBkyBNnZ2Trr/Pnnn3j22Wfh4OAAPz8/zJo1Sy+W9evXo169enBwcEDjxo2xbds2o7/fxzV9+nS0bNkSlSpVgqenJ3r27Inz58/rrJObm4sRI0agSpUqcHFxQa9evZCSkqKzzrVr19C9e3c4OTnB09MT48aNQ35+vs46v/76K1q0aAGFQoGgoCAsW7ZML56K/hlduHAhmjRpIl14LDw8HNu3b5eWM1fFmzFjBmQyGUaPHi2NMV8PTZ48GTKZTOerXr160nLmSt+NGzfw6quvokqVKnB0dETjxo1x7NgxafkT9/de0GNbs2aNsLe3F999953466+/xBtvvCHc3d1FSkqKpUMzqm3btomPP/5YbNiwQQAQGzdu1Fk+Y8YM4ebmJjZt2iROnTol/u///k8EBgaK+/fvS+t06dJFNG3aVBw5ckT89ttvIigoSPTt21danpGRIby8vET//v3FmTNnxA8//CAcHR3F4sWLpXUOHTokbGxsxKxZs8TZs2fFhAkThJ2dnTh9+rTJc2CIzp07i7i4OHHmzBlx8uRJ0a1bN1GzZk2RnZ0trfPWW28JPz8/sWfPHnHs2DHRqlUr0bp1a2l5fn6+aNSokYiIiBAnTpwQ27ZtE1WrVhXjx4+X1rl8+bJwcnISY8aMEWfPnhXz5s0TNjY2Ij4+XlrnSfiMbt68WWzdulX8888/4vz58+Kjjz4SdnZ24syZM0II5qo4R48eFQEBAaJJkybinXfekcaZr4diYmJEw4YNxa1bt6Sv//77T1rOXOm6e/eu8Pf3FwMHDhQJCQni8uXLYseOHeLixYvSOk/a33sWO0bwzDPPiBEjRkiP1Wq18PX1FdOnT7dgVKb1aLGj0WiEt7e3+Oyzz6Sx9PR0oVAoxA8//CCEEOLs2bMCgEhMTJTW2b59u5DJZOLGjRtCCCG+/vpr4eHhIZRKpbTOBx98IOrWrSs9joqKEt27d9eJJywsTAwdOtSo79HYUlNTBQCxf/9+IURBfuzs7MT69euldc6dOycAiMOHDwshCgpMuVwukpOTpXUWLlwoXF1dpRy9//77omHDhjqv1bt3b9G5c2fp8ZP6GfXw8BDffvstc1WMrKwsERwcLHbt2iXatWsnFTvMl66YmBjRtGnTIpcxV/o++OAD0bZt22KXP4l/73kY6zHl5eXh+PHjiIiIkMbkcjkiIiJw+PBhC0ZmXklJSUhOTtbJg5ubG8LCwqQ8HD58GO7u7ggNDZXWiYiIgFwuR0JCgrTOc889B3t7e2mdzp074/z580hLS5PWKfw62nUqer4zMjIAAJUrVwZQcKNblUql817q1auHmjVr6uSscePG8PLyktbp3LkzMjMz8ddff0nrlJSPJ/EzqlarsWbNGty7dw/h4eHMVTFGjBiB7t27670n5kvfhQsX4Ovri1q1aqF///64du0aAOaqKJs3b0ZoaCheeeUVeHp6onnz5liyZIm0/En8e89i5zHdvn0barVa55cAALy8vJCcnGyhqMxP+15LykNycjI8PT11ltva2qJy5co66xS1jcKvUdw6FTnfGo0Go0ePRps2baT7uiUnJ8Pe3h7u7u466z6as/LmIzMzE/fv33+iPqOnT5+Gi4sLFAoF3nrrLWzcuBENGjRgroqwZs0a/PHHH5g+fbreMuZLV1hYGJYtW4b4+HgsXLgQSUlJePbZZ5GVlcVcFeHy5ctYuHAhgoODsWPHDgwbNgyjRo3C8uXLATyZf++fuNtFED2JRowYgTNnzuDgwYOWDqVCq1u3Lk6ePImMjAz8+OOPiI6Oxv79+y0dVoVz/fp1vPPOO9i1axccHBwsHU6F17VrV+n7Jk2aICwsDP7+/li3bh0cHR0tGFnFpNFoEBoaik8//RQA0Lx5c5w5cwaLFi1CdHS0haMrH3Z2HlPVqlVhY2OjN3M/JSUF3t7eForK/LTvtaQ8eHt7IzU1VWd5fn4+7t69q7NOUdso/BrFrVNR8z1y5Ej88ssv2LdvH2rUqCGNe3t7Iy8vD+np6TrrP5qz8ubD1dUVjo6OT9Rn1N7eHkFBQQgJCcH06dPRtGlTfPnll8zVI44fP47U1FS0aNECtra2sLW1xf79+/HVV1/B1tYWXl5ezFcJ3N3dUadOHVy8eJGfrSL4+PigQYMGOmP169eXDv09iX/vWew8Jnt7e4SEhGDPnj3SmEajwZ49exAeHm7ByMwrMDAQ3t7eOnnIzMxEQkKClIfw8HCkp6fj+PHj0jp79+6FRqNBWFiYtM6BAwegUqmkdXbt2oW6devCw8NDWqfw62jXqWj5FkJg5MiR2LhxI/bu3YvAwECd5SEhIbCzs9N5L+fPn8e1a9d0cnb69GmdPxq7du2Cq6ur9MeotHw8yZ9RjUYDpVLJXD2iQ4cOOH36NE6ePCl9hYaGon///tL3zFfxsrOzcenSJfj4+PCzVYQ2bdroXSbjn3/+gb+/P4An9O+9QdOZqUhr1qwRCoVCLFu2TJw9e1a8+eabwt3dXWfmvjXIysoSJ06cECdOnBAAxOzZs8WJEyfE1atXhRAFpyK6u7uLn3/+Wfz555+iR48eRZ6K2Lx5c5GQkCAOHjwogoODdU5FTE9PF15eXuK1114TZ86cEWvWrBFOTk56pyLa2tqKzz//XJw7d07ExMRUyFPPhw0bJtzc3MSvv/6qc8prTk6OtM5bb70latasKfbu3SuOHTsmwsPDRXh4uLRce8prp06dxMmTJ0V8fLyoVq1akae8jhs3Tpw7d04sWLCgyFNeK/pn9MMPPxT79+8XSUlJ4s8//xQffvihkMlkYufOnUII5qo0hc/GEoL5Kuy9994Tv/76q0hKShKHDh0SERERomrVqiI1NVUIwVw96ujRo8LW1lZMmzZNXLhwQXz//ffCyclJrFq1SlrnSft7z2LHSObNmydq1qwp7O3txTPPPCOOHDli6ZCMbt++fQKA3ld0dLQQouB0xIkTJwovLy+hUChEhw4dxPnz53W2cefOHdG3b1/h4uIiXF1dxaBBg0RWVpbOOqdOnRJt27YVCoVCVK9eXcyYMUMvlnXr1ok6deoIe3t70bBhQ7F161aTve/yKipXAERcXJy0zv3798Xw4cOFh4eHcHJyEi+++KK4deuWznauXLkiunbtKhwdHUXVqlXFe++9J1Qqlc46+/btE82aNRP29vaiVq1aOq+hVdE/o4MHDxb+/v7C3t5eVKtWTXTo0EEqdIRgrkrzaLHDfD3Uu3dv4ePjI+zt7UX16tVF7969da4Zw1zp27Jli2jUqJFQKBSiXr164ptvvtFZ/qT9vZcJIYRhvSAiIiKiJwfn7BAREZFVY7FDREREVo3FDhEREVk1FjtERERk1VjsEBERkVVjsUNERERWjcUOERERWTUWO0RW7MqVK5DJZDh58qSlQ5H8/fffaNWqFRwcHNCsWTOzvW5AQADmzp1b5vV//fVXyGQyvXsmPW0GDhyInj17WjoMosfCYofIhAYOHAiZTIYZM2bojG/atAkymcxCUVlWTEwMnJ2dcf78eb173gCATCYr8Wvy5Mnlet3ExES8+eabZV6/devWuHXrFtzc3Mr1eoZYsmQJmjZtChcXF7i7u6N58+aYPn26yV+X6Glha+kAiKydg4MDZs6ciaFDh0o3t3vS5eXlwd7evlzPvXTpErp37y7dVPBRt27dkr5fu3YtJk2apHNTQhcXF+l7IQTUajVsbUv/U1atWjWD4rS3tzfL3ai/++47jB49Gl999RXatWsHpVKJP//8E2fOnDH5axM9LdjZITKxiIgIeHt7l/g/9cmTJ+sd0pk7dy4CAgKkx9rDCZ9++im8vLzg7u6OKVOmID8/H+PGjUPlypVRo0YNxMXF6W3/77//RuvWreHg4IBGjRph//79OsvPnDmDrl27wsXFBV5eXnjttddw+/ZtaXn79u0xcuRIjB49GlWrVkXnzp2LfB8ajQZTpkxBjRo1oFAo0KxZM8THx0vLZTIZjh8/jilTphTbpfH29pa+3NzcIJPJpMd///03KlWqhO3btyMkJAQKhQIHDx7EpUuX0KNHD3h5ecHFxQUtW7bE7t27dbb76GEsmUyGb7/9Fi+++CKcnJwQHByMzZs3S8sfPYy1bNkyuLu7Y8eOHahfvz5cXFzQpUsXneIsPz8fo0aNgru7O6pUqYIPPvgA0dHRJR4G2rx5M6KiojBkyBAEBQWhYcOG6Nu3L6ZNmyatk5iYiI4dO6Jq1apwc3NDu3bt8Mcff+hsRyaTYfHixXjhhRfg5OSE+vXr4/Dhw7h48SLat28PZ2dntG7dGpcuXZKeo/3cLV68GH5+fnByckJUVBQyMjKKjVej0WD69OkIDAyEo6MjmjZtih9//FFanpaWhv79+6NatWpwdHREcHBwkZ9JInNisUNkYjY2Nvj0008xb948/Pvvv4+1rb179+LmzZs4cOAAZs+ejZiYGLzwwgvw8PBAQkIC3nrrLQwdOlTvdcaNG4f33nsPJ06cQHh4OCIjI3Hnzh0AQHp6Op5//nk0b94cx44dQ3x8PFJSUhAVFaWzjeXLl8Pe3h6HDh3CokWLiozvyy+/xBdffIHPP/8cf/75Jzp37oz/+7//w4ULFwAUdG0aNmyI9957D7du3cLYsWPLlYcPP/wQM2bMwLlz59CkSRNkZ2ejW7du2LNnD06cOIEuXbogMjIS165dK3E7sbGxiIqKwp9//olu3bqhf//+uHv3brHr5+Tk4PPPP8fKlStx4MABXLt2Tec9zJw5E99//z3i4uJw6NAhZGZmYtOmTSXG4O3tjSNHjuDq1avFrpOVlYXo6GgcPHgQR44cQXBwMLp164asrCyd9aZOnYoBAwbg5MmTqFevHvr164ehQ4di/PjxOHbsGIQQGDlypM5zLl68iHXr1mHLli2Ij4/HiRMnMHz48GJjmT59OlasWIFFixbhr7/+wrvvvotXX31VKqAnTpyIs2fPYvv27Th37hwWLlyIqlWrlpgDIpMz+NahRFRm0dHRokePHkIIIVq1aiUGDx4shBBi48aNovCvX0xMjGjatKnOc+fMmSP8/f11tuXv7y/UarU0VrduXfHss89Kj/Pz84Wzs7P44YcfhBBCJCUlCQA6dxJWqVSiRo0aYubMmUIIIaZOnSo6deqk89rXr18XAKS7GLdr1040b9681Pfr6+srpk2bpjPWsmVLMXz4cOlx06ZNRUxMTKnbEkKIuLg44ebmJj3et2+fACA2bdpU6nMbNmwo5s2bJz329/cXc+bMkR4DEBMmTJAeZ2dnCwBi+/btOq+VlpYmxQJA527ZCxYsEF5eXtJjLy8v8dlnn0mP8/PzRc2aNaXPQFFu3rwpWrVqJQCIOnXqiOjoaLF27Vqdn/Oj1Gq1qFSpktiyZUux7+fw4cMCgFi6dKk09sMPPwgHBwfpcUxMjLCxsRH//vuvNLZ9+3Yhl8ulu34X/gzn5uYKJycn8fvvv+vEM2TIENG3b18hhBCRkZFi0KBBxcZOZAns7BCZycyZM7F8+XKcO3eu3Nto2LAh5PKHv7ZeXl5o3Lix9NjGxgZVqlRBamqqzvPCw8Ol721tbREaGirFcerUKezbtw8uLi7SV7169QBA55BHSEhIibFlZmbi5s2baNOmjc54mzZtHus9FyU0NFTncXZ2NsaOHYv69evD3d0dLi4uOHfuXKmdnSZNmkjfOzs7w9XVVS93hTk5OaF27drSYx8fH2n9jIwMpKSk4JlnnpGW29jYlJo3Hx8fHD58GKdPn8Y777yD/Px8REdHo0uXLtBoNACAlJQUvPHGGwgODoabmxtcXV2RnZ2t9/4Kvx8vLy8A0Pl8eHl5ITc3F5mZmdJYzZo1Ub16delxeHg4NBqNzjwprYsXLyInJwcdO3bU+bysWLFC+qwMGzYMa9asQbNmzfD+++/j999/L/H9E5kDJygTmclzzz2Hzp07Y/z48Rg4cKDOMrlcDiGEzphKpdLbhp2dnc5jmUxW5Jh2J1kW2dnZiIyMxMyZM/WW+fj4SN87OzuXeZum9mgsY8eOxa5du/D5558jKCgIjo6OePnll5GXl1fidgzNXVHrP/pzK69GjRqhUaNGGD58ON566y08++yz2L9/P/73v/8hOjoad+7cwZdffgl/f38oFAqEh4frvb/C8WnP9itqzJDPR2HZ2dkAgK1bt+oUSACgUCgAAF27dsXVq1exbds27Nq1Cx06dMCIESPw+eefl+s1iYyBnR0iM5oxYwa2bNmCw4cP64xXq1YNycnJOjtOY14b58iRI9L3+fn5OH78OOrXrw8AaNGiBf766y8EBAQgKChI58uQAsfV1RW+vr44dOiQzvihQ4fQoEED47yRYhw6dAgDBw7Eiy++iMaNG8Pb2xtXrlwx6Ws+ys3NDV5eXkhMTJTG1Gq13kTistDm6969ewAK3t+oUaPQrVs3NGzYEAqFQmcC+eO4du0abt68KT0+cuQI5HI56tatW2RcCoUC165d0/us+Pn5SetVq1YN0dHRWLVqFebOnYtvvvnGKLESlRc7O0Rm1LhxY/Tv3x9fffWVznj79u3x33//YdasWXj55ZcRHx+P7du3w9XV1Sivu2DBAgQHB6N+/fqYM2cO0tLSMHjwYADAiBEjsGTJEvTt2xfvv/8+KleujIsXL2LNmjX49ttvYWNjU+bXGTduHGJiYlC7dm00a9YMcXFxOHnyJL7//nujvI/iBAcHY8OGDYiMjIRMJsPEiRPL3b14HG+//TamT5+OoKAg1KtXD/PmzUNaWlqJ11QaNmwYfH198fzzz6NGjRq4desWPvnkE1SrVk06/BgcHIyVK1ciNDQUmZmZGDduHBwdHY0Ss4ODA6Kjo/H5558jMzMTo0aNQlRUVJGn3VeqVAljx47Fu+++C41Gg7Zt2yIjIwOHDh2Cq6sroqOjMWnSJISEhKBhw4ZQKpX45ZdfpMKayFLY2SEysylTpujtiOvXr4+vv/4aCxYsQNOmTXH06NFyn6lUlBkzZmDGjBlo2rQpDh48iM2bN0tnyGi7MWq1Gp06dULjxo0xevRouLu768wPKotRo0ZhzJgxeO+999C4cWPEx8dj8+bNCA4ONtp7Kcrs2bPh4eGB1q1bIzIyEp07d0aLFi1M+ppF+eCDD9C3b18MGDAA4eHhcHFxQefOneHg4FDscyIiInDkyBG88sorqFOnDnr16gUHBwfs2bMHVapUAQAsXboUaWlpaNGiBV577TWMGjUKnp6eRok5KCgIL730Erp164ZOnTqhSZMm+Prrr4tdf+rUqZg4cSKmT5+O+vXro0uXLti6dSsCAwMBFFyfaPz48WjSpAmee+452NjYYM2aNUaJlai8ZMJYB5yJiEiHRqNB/fr1ERUVhalTp1o6HD2TJ0/Gpk2bKtTtRIhMgYexiIiM5OrVq9i5c6d0JeT58+cjKSkJ/fr1s3RoRE81HsYiIjISuVyOZcuWoWXLlmjTpg1Onz6N3bt3c84KkYXxMBYRERFZNXZ2iIiIyKqx2CEiIiKrxmKHiIiIrBqLHSIiIrJqLHaIiIjIqrHYISIiIqvGYoeIiIisGosdIiIismosdoiIiMiq/T9LDvnVVoYR8gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define function to load MNIST data\n",
    "def load_data(batch_size, num_samples):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    indices = np.random.choice(len(trainset), num_samples, replace=False)\n",
    "    trainset = torch.utils.data.Subset(trainset, indices)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "# Define function to train the model\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "# Define function to evaluate the model\n",
    "def evaluate_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on test set: %.2f %%' % (accuracy))\n",
    "    return accuracy\n",
    "\n",
    "# Main code\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "num_training_samples = [600, 1800, 6000, 18000, 60000]\n",
    "accuracies = []\n",
    "\n",
    "for num_samples in num_training_samples:\n",
    "    print(f\"\\nTraining with {num_samples} samples:\")\n",
    "    trainloader, testloader = load_data(batch_size, num_samples)\n",
    "    model = LeNet()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    train_model(model, trainloader, criterion, optimizer, epochs, device)\n",
    "    accuracy = evaluate_model(model, testloader, device)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(num_training_samples, accuracies, marker='o')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Performance vs Number of Training Samples')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# Define the Vision Transformer (ViT) model\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=512, nhead=8, num_encoder_layers=6):\n",
    "        super(ViT, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.transformer_encoder = TransformerEncoder(TransformerEncoderLayer(d_model=d_model, nhead=nhead), num_layers=num_encoder_layers)\n",
    "        self.fc = nn.Linear(input_dim * d_model, num_classes)\n",
    "\n",
    "    # Modify the forward method of the ViT model to handle input tensors of appropriate shape\n",
    "    def forward(self, x):\n",
    "    # Flatten the input tensor if it is not already flattened\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(-1, 28*28)  # Assuming MNIST images are of size 28x28\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x    \n",
    "\n",
    "\n",
    "# Define function to load MNIST data\n",
    "def load_data(batch_size, num_samples):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    indices = torch.randperm(len(trainset))[:num_samples]\n",
    "    trainset = torch.utils.data.Subset(trainset, indices)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Flatten the images\n",
    "    trainloader.dataset.data = trainloader.dataset.dataset.data.view(len(trainloader.dataset), -1)\n",
    "    testloader.dataset.data = testloader.dataset.data.view(len(testloader.dataset), -1)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "\n",
    "# Define function to train the model\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):  \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, Loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "# Define function to evaluate the model\n",
    "def evaluate_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on test set: %.2f %%' % (accuracy))\n",
    "    return accuracy\n",
    "\n",
    "# Main code\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "num_training_samples = [6000, 60000]\n",
    "accuracies_vit = []\n",
    "\n",
    "for num_samples in num_training_samples:\n",
    "    print(f\"\\nTraining with {num_samples} samples:\")\n",
    "    trainloader, testloader = load_data(batch_size, num_samples)\n",
    "    model = ViT(input_dim=28*28, num_classes=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    train_model(model, trainloader, criterion, optimizer, epochs, device)\n",
    "    accuracy = evaluate_model(model, testloader, device)\n",
    "    accuracies_vit.append(accuracy)\n",
    "\n",
    "print(\"Accuracy with ViT model:\", accuracies_vit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
